{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13943031,"sourceType":"datasetVersion","datasetId":8886325}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!ls\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T21:51:35.587075Z","iopub.execute_input":"2025-12-08T21:51:35.587278Z","iopub.status.idle":"2025-12-08T21:51:35.756750Z","shell.execute_reply.started":"2025-12-08T21:51:35.587263Z","shell.execute_reply":"2025-12-08T21:51:35.755794Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch, torchvision\nprint(torch.__version__, torchvision.__version__)\nprint(\"CUDA available:\", torch.cuda.is_available())\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T21:40:00.906511Z","iopub.execute_input":"2025-12-08T21:40:00.906746Z","iopub.status.idle":"2025-12-08T21:40:00.911649Z","shell.execute_reply.started":"2025-12-08T21:40:00.906720Z","shell.execute_reply":"2025-12-08T21:40:00.910899Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport numpy as np\nimport uuid\n\n\nclass GenericObject:\n    \"\"\"\n    Generic object data.\n    \"\"\"\n    def __init__(self):\n        self.id = uuid.uuid4()\n        self.bb = (-1, -1, -1, -1)\n        self.category= -1\n        self.score = -1\n\nclass GenericImage:\n    \"\"\"\n    Generic image data.\n    \"\"\"\n    def __init__(self, filename):\n        self.filename = filename\n        self.tile = np.array([-1, -1, -1, -1])  # (pt_x, pt_y, pt_x+width, pt_y+height)\n        self.objects = list([])\n\n    def add_object(self, obj: GenericObject):\n        self.objects.append(obj)\n\ncategories = {0: 'Small car', 1: 'Bus', 2: 'Truck', 3: 'Building'}\n\njson_file = '/kaggle/input/xview-detection/xview_det_train.json'\nwith open(json_file) as ifs:\n    json_data = json.load(ifs)\n\ncounts = dict.fromkeys(categories.values(), 0)\nanns = []\nfor json_img in json_data['images'].values():\n    image = GenericImage(json_img['filename'])\n    image.tile = np.array([0, 0, json_img['width'], json_img['height']])\n    for json_ann in [elem for elem in json_data['annotations'].values()\n                     if elem['image_id'] == json_img['image_id']]:\n        obj = GenericObject()\n        obj.id = json_ann['image_id']\n        obj.bb = (\n            int(json_ann['bbox'][0]),\n            int(json_ann['bbox'][1]),\n            int(json_ann['bbox'][2]),\n            int(json_ann['bbox'][3]),\n        )\n        obj.category = json_ann['category_id']\n        counts[obj.category] += 1\n        image.add_object(obj)\n    anns.append(image)\n\nprint(counts)\n\nfrom sklearn.model_selection import train_test_split\n\nanns_train, anns_valid = train_test_split(\n    anns, test_size=0.1, random_state=1, shuffle=True\n)\n\n\n# Map category_id -> 0-based index\ncatid_to_idx = {cat_id: i for i, cat_id in categories.items()}\nnum_classes = len(catid_to_idx)\n\ndef build_torch_annotations(anns, catid_to_idx):\n    torch_anns = []\n    for img_ann in anns:\n        boxes = []\n        labels = []\n        for obj in img_ann.objects:\n            boxes.append(list(obj.bb))  # [x1, y1, x2, y2]\n            cls_idx = catid_to_idx[obj.category]   # 0..num_classes-1\n            labels.append(cls_idx + 1)             # 1..num_classes (0 = background)\n        torch_anns.append({\n            \"file_name\": img_ann.filename,\n            \"boxes\": boxes,\n            \"labels\": labels,\n        })\n    return torch_anns\n\ntrain_annotations = build_torch_annotations(anns_train, catid_to_idx)\nvalid_annotations = build_torch_annotations(anns_valid, catid_to_idx)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T21:40:00.912569Z","iopub.execute_input":"2025-12-08T21:40:00.912972Z","iopub.status.idle":"2025-12-08T21:46:05.243173Z","shell.execute_reply.started":"2025-12-08T21:40:00.912943Z","shell.execute_reply":"2025-12-08T21:46:05.242629Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q transformers timm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T21:50:32.047630Z","iopub.execute_input":"2025-12-08T21:50:32.047923Z","iopub.status.idle":"2025-12-08T21:51:34.261427Z","shell.execute_reply.started":"2025-12-08T21:50:32.047902Z","shell.execute_reply":"2025-12-08T21:51:34.260461Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import DetrImageProcessor, DetrForObjectDetection\n\nid2label = {i: str(i) for i in range(1, num_classes + 1)}\nlabel2id = {v: k for k, v in id2label.items()}\n\nprocessor = DetrImageProcessor.from_pretrained(\n    \"facebook/detr-resnet-50\", \n    size=640,                # por ejemplo\n    max_size=640\n)\n\nmodel = DetrForObjectDetection.from_pretrained(\n    \"facebook/detr-resnet-50\",\n    num_labels=num_classes,   # SIN background, DETR a√±ade \"no object\"\n    id2label=id2label,\n    label2id=label2id,\n    ignore_mismatched_sizes=True,\n)\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T21:51:44.369033Z","iopub.execute_input":"2025-12-08T21:51:44.369385Z","iopub.status.idle":"2025-12-08T21:51:45.641540Z","shell.execute_reply.started":"2025-12-08T21:51:44.369353Z","shell.execute_reply":"2025-12-08T21:51:45.640752Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as F\n\nclass MyDetectionDataset(Dataset):\n    def __init__(self, annotations, image_dir=\"\", transforms=None):\n        self.annotations = annotations\n        self.image_dir = image_dir\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, idx):\n        ann = self.annotations[idx]\n        file_name = ann[\"file_name\"]\n\n        if os.path.isabs(file_name) or self.image_dir == \"\":\n            img_path = file_name\n        else:\n            img_path = os.path.join(self.image_dir, file_name)\n\n        img = Image.open(img_path).convert(\"RGB\")\n\n        boxes = torch.as_tensor(ann[\"boxes\"], dtype=torch.float32)\n        labels = torch.as_tensor(ann[\"labels\"], dtype=torch.int64)\n\n        # üîß Filter out degenerate (zero / negative width or height) boxes\n        if boxes.numel() > 0:\n            x1 = boxes[:, 0]\n            y1 = boxes[:, 1]\n            x2 = boxes[:, 2]\n            y2 = boxes[:, 3]\n\n            #discard boxes with zero height or width \n            keep = (x2 > x1) & (y2 > y1)\n            boxes = boxes[keep]\n            labels = labels[keep]\n\n        # if everything got filtered, still return empty tensors (allowed)\n        if boxes.numel() == 0:\n            boxes = torch.zeros((0, 4), dtype=torch.float32)\n            labels = torch.zeros((0,), dtype=torch.int64)\n\n        target = {\n            \"boxes\": boxes,\n            \"labels\": labels,\n            \"image_id\": torch.tensor([idx]),\n        }\n\n        if self.transforms is not None:\n            img = self.transforms(img)\n\n        return img, target\n\n\ntrain_tfms = T.Compose([T.ToTensor()])\nval_tfms = T.Compose([T.ToTensor()])\n\nimage_root = \"/kaggle/input/xview-detection\" \n\ntrain_dataset = MyDetectionDataset(train_annotations, image_dir=image_root, transforms=train_tfms)\nvalid_dataset = MyDetectionDataset(valid_annotations, image_dir=image_root, transforms=val_tfms)\n\n\n\nclass DetrDataset(torch.utils.data.Dataset):\n    def __init__(self, base_dataset, processor):\n        self.base_dataset = base_dataset\n        self.processor = processor\n\n    def __len__(self):\n        return len(self.base_dataset)\n\n    def __getitem__(self, idx):\n        img, target = self.base_dataset[idx]   # img: [C,H,W] tensor, target con boxes xyxy\n\n        # pasar a PIL para que el processor se apa√±e bien con tama√±os\n        pil_img = F.to_pil_image(img)\n\n        # cajas xyxy -> cxcywh relativas\n        boxes = target[\"boxes\"].clone().numpy()  # (N, 4)\n        labels = target[\"labels\"].clone().numpy()\n\n        w, h = pil_img.size\n        # xyxy -> cxcywh en pixeles\n        x1, y1, x2, y2 = boxes[:,0], boxes[:,1], boxes[:,2], boxes[:,3]\n        cx = (x1 + x2) / 2.0\n        cy = (y1 + y2) / 2.0\n        bw = (x2 - x1)\n        bh = (y2 - y1)\n\n        # normalizar a [0,1]\n        boxes_cxcywh = np.stack([cx / w, cy / h, bw / w, bh / h], axis=-1)\n\n        encoding = self.processor(\n            images=pil_img,\n            return_tensors=\"pt\"\n        )\n\n        # labels en formato que espera DetrForObjectDetection\n        encoding[\"labels\"] = [{\n            \"class_labels\": torch.as_tensor(labels, dtype=torch.long),\n            \"boxes\": torch.as_tensor(boxes_cxcywh, dtype=torch.float32),\n        }]\n\n        # quitar dimensi√≥n batch (processor devuelve [1,...])\n        encoding = {k: v[0] if isinstance(v, torch.Tensor) and v.dim() > 1 else v\n                    for k, v in encoding.items()}\n\n        return encoding\n\ndetr_train_dataset = DetrDataset(train_dataset, processor)\ndetr_valid_dataset = DetrDataset(valid_dataset, processor)\n\n\ndef detr_collate_fn(batch):\n    # batch: lista de dicts\n    pixel_values = torch.stack([b[\"pixel_values\"] for b in batch])\n    labels = [b[\"labels\"][0] for b in batch]  # cada uno es un dict con class_labels/boxes\n    return {\"pixel_values\": pixel_values, \"labels\": labels}\n\ndetr_train_loader = DataLoader(\n    detr_train_dataset,\n    batch_size=2,\n    shuffle=True,\n    num_workers=0,\n    collate_fn=detr_collate_fn,\n)\n\ndetr_valid_loader = DataLoader(\n    detr_valid_dataset,\n    batch_size=2,\n    shuffle=False,\n    num_workers=0,\n    collate_fn=detr_collate_fn,\n)\n\n\ndef compute_discard_stats(annotations):\n    total = len(annotations)\n    discarded = 0\n    total_boxes = 0\n    discarded_boxes = 0\n\n    for ann in annotations:\n        valid = 0\n        for (x1, y1, x2, y2) in ann[\"boxes\"]:\n            total_boxes += 1\n            if x2 > x1 and y2 > y1:\n                valid += 1\n            else:\n                discarded_boxes += 1\n\n        if valid == 0:\n            discarded += 1\n\n    return {\n        \"total_images\": total,\n        \"images_discarded\": discarded,\n        \"images_discarded_pct\": 100 * discarded / total,\n        \"total_boxes\": total_boxes,\n        \"boxes_discarded\": discarded_boxes,\n        \"boxes_discarded_pct\": 100 * discarded_boxes / total_boxes if total_boxes > 0 else 0,\n    }\n\ntrain_stats = compute_discard_stats(train_annotations)\nvalid_stats = compute_discard_stats(valid_annotations)\n\ntrain_stats, valid_stats","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T21:52:05.225485Z","iopub.execute_input":"2025-12-08T21:52:05.226120Z","iopub.status.idle":"2025-12-08T21:52:05.351280Z","shell.execute_reply.started":"2025-12-08T21:52:05.226092Z","shell.execute_reply":"2025-12-08T21:52:05.350711Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nEntrenamiento finalizado.\")\n\n# Cargar el mejor modelo\nmodel.load_state_dict(torch.load(\"/kaggle/working/best_detr_weights.pth\", map_location=device))\nmodel.to(device)\nmodel.eval()\n\nprint(\"‚úî Mejor modelo cargado correctamente.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T21:46:05.274863Z","iopub.status.idle":"2025-12-08T21:46:05.275136Z","shell.execute_reply.started":"2025-12-08T21:46:05.275016Z","shell.execute_reply":"2025-12-08T21:46:05.275029Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\nwith open(\"/kaggle/working/best_detr-history.json\", \"w\") as f:\n    json.dump(history_detr, f)\npath = \"/kaggle/working/best_detr-full.pth\"\n\ntorch.save(model, path)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T21:46:05.276182Z","iopub.status.idle":"2025-12-08T21:46:05.276490Z","shell.execute_reply.started":"2025-12-08T21:46:05.276309Z","shell.execute_reply":"2025-12-08T21:46:05.276323Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"device = torch.device(\"cuda\")  \npath = \"/kaggle/working/FasterRCNN-full.pth\"\nmodel = torch.load(path, map_location=device, weights_only=False)\nmodel.to(device)\nmodel.eval()\"\"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T21:46:05.277348Z","iopub.status.idle":"2025-12-08T21:46:05.277684Z","shell.execute_reply.started":"2025-12-08T21:46:05.277507Z","shell.execute_reply":"2025-12-08T21:46:05.277521Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.colors as col\nimport numpy as np\n%matplotlib inline\n\ndef area_intersection(boxes, box):\n    xmin = np.maximum(np.min(boxes[:, 0::2], axis=1), np.min(box[0::2]))\n    ymin = np.maximum(np.min(boxes[:, 1::2], axis=1), np.min(box[1::2]))\n    xmax = np.minimum(np.max(boxes[:, 0::2], axis=1), np.max(box[0::2]))\n    ymax = np.minimum(np.max(boxes[:, 1::2], axis=1), np.max(box[1::2]))\n    w = np.maximum(xmax - xmin + 1.0, 0.0)\n    h = np.maximum(ymax - ymin + 1.0, 0.0)\n    return w * h\n\ndef area_union(boxes, box):\n    area_anns = (np.max(box[0::2])-np.min(box[0::2])+1.0) * (np.max(box[1::2])-np.min(box[1::2])+1.0)\n    area_pred = (np.max(boxes[:, 0::2], axis=1)-np.min(boxes[:, 0::2], axis=1)+1.0) * (np.max(boxes[:, 1::2], axis=1)-np.min(boxes[:, 1::2], axis=1)+1.0)\n    return area_anns + area_pred - area_intersection(boxes, box)\n\ndef calc_iou(boxes, box):\n    iou = area_intersection(boxes, box) / area_union(boxes, box)\n    max_value = np.max(iou)\n    max_index = np.argmax(iou)\n    return max_value, max_index\n\ndef calc_ap(rec, prec):\n    # First append sentinel values at the end\n    mrec = np.concatenate(([0.0], rec, [1.0]))\n    mpre = np.concatenate(([0.0], prec, [0.0]))\n    # Compute the precision envelope\n    for i in range(mpre.size-1, 0, -1):\n        mpre[i-1] = np.maximum(mpre[i-1], mpre[i])\n    # To calculate area under PR curve, look for points where X axis (recall) changes value\n    i = np.where(mrec[1:] != mrec[:-1])[0]\n    # and sum (\\Delta recall) * prec\n    ap = np.sum((mrec[i+1] - mrec[i]) * mpre[i+1])\n    return ap\n\ndef draw_confusion_matrix(cm, categories):\n    # Draw confusion matrix\n    fig = plt.figure(figsize=[6.4*pow(len(categories), 0.5), 4.8*pow(len(categories), 0.5)])\n    ax = fig.add_subplot(111)\n    cm = cm.astype('float') / np.maximum(cm.sum(axis=1)[:, np.newaxis], np.finfo(np.float64).eps)\n    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.get_cmap('Blues'))\n    ax.figure.colorbar(im, ax=ax)\n    ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]), xticklabels=categories, yticklabels=categories, ylabel='Annotation', xlabel='Prediction')\n    # Rotate the tick labels and set their alignment\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n    # Loop over data dimensions and create text annotations\n    thresh = cm.max() / 2.0\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], '.2f'), ha=\"center\", va=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=int(20-pow(len(categories), 0.5)))\n    fig.tight_layout()\n    plt.show()\n\ndef draw_precision_recall(precisions, recalls, categories):\n    # Draw precision-recall curves for each category\n    fig = plt.figure(figsize=[6.4*pow(len(categories), 0.5), 4.8*pow(len(categories), 0.5)])\n    ax = fig.add_subplot(111)\n    plt.axis([0, 1, 0, 1])\n    c_dark = list(filter(lambda x: x.startswith('dark'), col.cnames.keys()))\n    aps = []\n    # Compare categories for a specific algorithm\n    for idx in range(len(categories)):\n        plt.plot(recalls[idx], precisions[idx], color=c_dark[idx], label=categories[idx], linewidth=4.0)\n        aps.append(calc_ap(recalls[idx], precisions[idx]))\n    handles, labels = ax.get_legend_handles_labels()\n    labels = [str(val + ' [' + \"{:.3f}\".format(aps[idx]) + ']') for idx, val in enumerate(labels)]\n    handles = [h for (ap, h) in sorted(zip(aps, handles), key=lambda x: x[0], reverse=True)]\n    labels = [l for (ap, l) in sorted(zip(aps, labels), key=lambda x: x[0], reverse=True)]\n    leg = plt.legend(handles, labels, loc='upper right')\n    leg.set_zorder(100)\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.grid(\"on\", linestyle=\"--\", linewidth=2.0)\n    fig.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T21:46:05.279127Z","iopub.status.idle":"2025-12-08T21:46:05.279371Z","shell.execute_reply.started":"2025-12-08T21:46:05.279240Z","shell.execute_reply":"2025-12-08T21:46:05.279249Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, recall_score, precision_score\n\niou_thresh = 0.5\n\niou_thresh = 0.5\n\ndef compute_eval_vectors(model, dataset, device, num_classes):\n    classes = list(range(1, num_classes + 1))  # 1..4\n\n    confidences = {cls: [] for cls in classes}\n    tps         = {cls: [] for cls in classes}\n\n    y_true_cls = []   # GT por objeto\n    y_pred_cls = []   # predicci√≥n por objeto (puede ser 0 o clase equivocada)\n\n    model.eval()\n    with torch.no_grad():\n        for img, target in tqdm(dataset, desc=\"Evaluating\"):\n            img_gpu = img.to(device)\n\n            output = model([img_gpu])[0]\n\n            pred_boxes  = output[\"boxes\"].cpu().numpy()\n            pred_scores = output[\"scores\"].cpu().numpy()\n            pred_labels = output[\"labels\"].cpu().numpy()     # 0..4\n\n            gt_boxes  = target[\"boxes\"].numpy()\n            gt_labels = target[\"labels\"].numpy()             # 1..4\n\n            n_gt = len(gt_boxes)\n            if n_gt == 0:\n                continue\n\n            # matriz IoU [num_preds x num_gt]\n            if len(pred_boxes) > 0:\n                inter = np.zeros((len(pred_boxes), n_gt))\n                uni   = np.zeros((len(pred_boxes), n_gt))\n                for p_idx, pb in enumerate(pred_boxes):\n                    inter[p_idx] = area_intersection(gt_boxes, pb)\n                    uni[p_idx]   = area_union(gt_boxes, pb)\n                ious = inter / np.maximum(uni, np.finfo(np.float64).eps)\n            else:\n                ious = np.zeros((0, n_gt))\n\n            # para cada GT, buscamos la pred con mayor score e IoU >= thresh\n            gt_best_pred = np.full(n_gt, -1, dtype=int)\n            gt_best_score = np.full(n_gt, -1.0, dtype=float)\n\n            for p_idx in range(len(pred_boxes)):\n                for g_idx in range(n_gt):\n                    if ious[p_idx, g_idx] >= iou_thresh:\n                        if pred_scores[p_idx] > gt_best_score[g_idx]:\n                            gt_best_score[g_idx] = pred_scores[p_idx]\n                            gt_best_pred[g_idx] = p_idx\n\n            # ahora, por GT, definimos y_true_cls / y_pred_cls\n            for g_idx, gl in enumerate(gt_labels):\n                y_true_cls.append(gl)\n                p_idx = gt_best_pred[g_idx]\n\n                if p_idx == -1:\n                    # no hay pred con IoU suficiente -> background\n                    y_pred_cls.append(0)\n                else:\n                    # usamos la CLASE PREDICHA (puede ser correcta o no)\n                    pl = pred_labels[p_idx]\n                    if pl < 0 or pl > num_classes:\n                        pl = 0\n                    y_pred_cls.append(pl)\n\n            # TP/FP por predicci√≥n (para AP/mAP)\n            used_gt = np.zeros(n_gt, dtype=bool)\n            order = np.argsort(-pred_scores)\n            for p_idx in order:\n                pb = pred_boxes[p_idx]\n                ps = pred_scores[p_idx]\n                pl = pred_labels[p_idx]\n\n                if pl == 0 or pl < 1 or pl > num_classes:\n                    continue\n\n                if n_gt > 0:\n                    ious_p = ious[p_idx]\n                    best_gt = np.argmax(ious_p)\n                    best_iou = ious_p[best_gt]\n                else:\n                    best_iou = 0.0\n\n                if best_iou >= iou_thresh and not used_gt[best_gt] and pl == gt_labels[best_gt]:\n                    used_gt[best_gt] = True\n                    tps[pl].append(1)\n                    confidences[pl].append(ps)\n                else:\n                    tps[pl].append(0)\n                    confidences[pl].append(ps)\n\n    return np.array(y_true_cls), np.array(y_pred_cls), confidences, tps\n\nNUM_CLASSES = 4\n\ny_true_cls, y_pred_cls, confidences, tps = compute_eval_vectors(model, valid_dataset, device, NUM_CLASSES)\n\nid2name = {i+1: name for i, name in categories.items()}  # {1:'Small car',...}\n\nclasses = list(range(1, NUM_CLASSES + 1))\n\n# Compute AP metric\neps = np.finfo(np.float64).eps\n\nprecision_list, recall_list, ap_list = [], [], []\n\nfor cls_id, cls_name in id2name.items():  # cls_id = 1..4\n    if cls_id not in confidences or len(confidences[cls_id]) == 0:\n        print(f'> {cls_name}: no predictions')\n        precision_list.append(np.array([0.0]))\n        recall_list.append(np.array([0.0]))\n        ap_list.append(0.0)\n        continue\n\n    # sort by confidence desc\n    sorted_ind = np.argsort(-np.array(confidences[cls_id]))\n    tp_sorted  = np.array(tps[cls_id])[sorted_ind]\n\n    tp = np.cumsum(tp_sorted, dtype=float)\n\n    # GTs of this class\n    recall = np.array([0.0]) if len(tp) == 0 else tp / np.maximum(np.sum(y_true_cls == cls_id), eps)\n    # preds of this class (1..len(tp))\n    precision = np.array([0.0]) if len(tp) == 0 else tp / np.maximum(np.arange(1, len(tp)+1), eps)\n\n    ap = calc_ap(recall, precision)\n\n    print('> %s: Recall: %.3f%% Precision: %.3f%% AP: %.3f%%' %\n          (cls_name, recall[-1]*100, precision[-1]*100, ap*100))\n\n    precision_list.append(precision)\n    recall_list.append(recall)\n    ap_list.append(ap)\n\nmean_ap = np.mean(ap_list)\n\n\n\n\nprint('mAccuracy: %.3f%%' % (accuracy_score(y_true_cls, y_pred_cls)*100))\nprint('mRecall: %.3f%%' % (recall_score(y_true_cls, y_pred_cls, average='macro', zero_division=1)*100))\nprint('mPrecision: %.3f%%' % (precision_score(y_true_cls, y_pred_cls, average='macro', zero_division=1)*100))\nprint('mAP: %.3f%%' % (mean_ap*100))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T21:46:05.280614Z","iopub.status.idle":"2025-12-08T21:46:05.280820Z","shell.execute_reply.started":"2025-12-08T21:46:05.280719Z","shell.execute_reply":"2025-12-08T21:46:05.280728Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom tqdm.auto import tqdm\n\nnum_fg_classes = len(catid_to_idx)      # 4\nnum_classes_cm = num_fg_classes + 1     # +1 for background index 0\n\ncm = np.zeros((num_classes_cm, num_classes_cm), dtype=np.int64)  # [gt, pred]\n\niou_thresh = 0.5\nscore_thresh = 0.5   # solo consideramos predicciones con score >= score_thresh\n\nmodel.eval()\n\nwith torch.no_grad():\n    for images, targets in tqdm(valid_loader, desc=\"Evaluating\"):\n        # asumiendo batch_size=1 en valid_loader\n        img = images[0].to(device)\n        gt = targets[0]\n\n        outputs = model([img])\n        output = outputs[0]\n\n        gt_boxes = gt[\"boxes\"].cpu().numpy()\n        gt_labels = gt[\"labels\"].cpu().numpy()     # 1..4\n\n        pred_boxes = output[\"boxes\"].cpu().numpy()\n        pred_scores = output[\"scores\"].cpu().numpy()\n        pred_labels = output[\"labels\"].cpu().numpy()   # 0..4\n\n        # filtrar por score\n        keep = pred_scores >= score_thresh\n        pred_boxes = pred_boxes[keep]\n        pred_scores = pred_scores[keep]\n        pred_labels = pred_labels[keep]\n\n        n_gt = len(gt_boxes)\n        n_pred = len(pred_boxes)\n\n        # si no hay GT ni preds, no aporta nada\n        if n_gt == 0 and n_pred == 0:\n            continue\n\n        # matriz IoU [num_pred x num_gt]\n        if n_gt > 0 and n_pred > 0:\n            inter = np.zeros((n_pred, n_gt))\n            uni   = np.zeros((n_pred, n_gt))\n            for p_idx, pb in enumerate(pred_boxes):\n                inter[p_idx] = area_intersection(gt_boxes, pb)\n                uni[p_idx]   = area_union(gt_boxes, pb)\n            ious = inter / np.maximum(uni, np.finfo(np.float64).eps)\n        else:\n            ious = np.zeros((n_pred, n_gt))\n\n        matched_gt = np.zeros(n_gt, dtype=bool)\n        matched_pred = np.zeros(n_pred, dtype=bool)\n\n        # Primero: asignaci√≥n tipo \"best match\" por GT\n        for g_idx in range(n_gt):\n            gl = int(gt_labels[g_idx])          # 1..4\n            gt_box = gt_boxes[g_idx]\n\n            if n_pred == 0:\n                continue\n\n            iou_vals = ious[:, g_idx]\n            best_idx = np.argmax(iou_vals)\n            best_iou = iou_vals[best_idx]\n\n            if best_iou >= iou_thresh:\n                pl = int(pred_labels[best_idx])  # 0..4\n\n                # clamp pred label to [0..num_fg_classes]\n                if pl < 0 or pl > num_fg_classes:\n                    pl = 0\n\n                cm[gl, pl] += 1\n\n                matched_gt[g_idx] = True\n                matched_pred[best_idx] = True\n            else:\n                # no pred con IOU suficiente -> FN (gt vs background)\n                cm[gl, 0] += 1\n                matched_gt[g_idx] = True  # already accounted as FN\n\n        # FNs para GTs que nunca tuvieron ning√∫n pred candidate (n_pred == 0 caso)\n        for g_idx in range(n_gt):\n            if not matched_gt[g_idx]:\n                gl = int(gt_labels[g_idx])\n                if gl < 0 or gl > num_fg_classes:\n                    gl = 0\n                cm[gl, 0] += 1\n\n        # FPs: predicciones sin GT asociado -> background vs pred class\n        for p_idx in range(n_pred):\n            if not matched_pred[p_idx]:\n                pl = int(pred_labels[p_idx])\n                if pl < 0 or pl > num_fg_classes:\n                    pl = 0\n                cm[0, pl] += 1\n\n# index 0 = background\n# 1..4 = Small car, Bus, Truck, Building\ncategory_names = [\"background\"] + [categories[i] for i in range(num_fg_classes)]\n\ndraw_confusion_matrix(cm, category_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T21:46:05.281722Z","iopub.status.idle":"2025-12-08T21:46:05.282033Z","shell.execute_reply.started":"2025-12-08T21:46:05.281885Z","shell.execute_reply":"2025-12-08T21:46:05.281897Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_pr_curves(model, dataset, device, iou_thresh=0.5, score_thresh=0.0):\n    model.eval()\n    num_classes = len(catid_to_idx)\n\n    # por clase: listas de (score, is_tp)\n    class_scores = [[] for _ in range(num_classes)]\n    class_tp = [[] for _ in range(num_classes)]\n    class_total_gt = [0 for _ in range(num_classes)]\n\n    with torch.no_grad():\n        for i in tqdm(range(len(dataset)), desc=\"PR eval\"):\n            img, target = dataset[i]\n            img_gpu = img.to(device)\n            pred = model([img_gpu])[0]\n\n            gt_boxes = target[\"boxes\"].numpy()\n            gt_labels = target[\"labels\"].numpy() - 1      # 0..C-1\n            pred_boxes = pred[\"boxes\"].cpu().numpy()\n            pred_scores = pred[\"scores\"].cpu().numpy()\n            pred_labels = pred[\"labels\"].cpu().numpy() - 1\n\n            # contar GTs por clase\n            for gl in gt_labels:\n                class_total_gt[int(gl)] += 1\n\n            # filtrar por score\n            keep = pred_scores >= score_thresh\n            pred_boxes = pred_boxes[keep]\n            pred_scores = pred_scores[keep]\n            pred_labels = pred_labels[keep]\n\n            # marcar GTs como no usados a√∫n\n            used_gt = np.zeros(len(gt_boxes), dtype=bool)\n\n            # ordenar predicciones por score desc\n            order = np.argsort(-pred_scores)\n            pred_boxes = pred_boxes[order]\n            pred_scores = pred_scores[order]\n            pred_labels = pred_labels[order]\n\n            # para cada pred, ver si es TP o FP\n            for pb, ps, pl in zip(pred_boxes, pred_scores, pred_labels):\n                if len(gt_boxes) == 0:\n                    class_scores[pl].append(ps)\n                    class_tp[pl].append(0)\n                    continue\n\n                ious = area_intersection(gt_boxes, pb) / area_union(gt_boxes, pb)\n                best_gt = np.argmax(ious)\n                best_iou = ious[best_gt]\n\n                if best_iou >= iou_thresh and (not used_gt[best_gt]) and (pl == gt_labels[best_gt]):\n                    used_gt[best_gt] = True\n                    class_scores[pl].append(ps)\n                    class_tp[pl].append(1)   # TP\n                else:\n                    class_scores[pl].append(ps)\n                    class_tp[pl].append(0)   # FP\n\n    # ahora calculamos precision/recall por clase\n    precisions = []\n    recalls = []\n\n    for c in range(num_classes):\n        if len(class_scores[c]) == 0 or class_total_gt[c] == 0:\n            precisions.append(np.array([0.0]))\n            recalls.append(np.array([0.0]))\n            continue\n\n        # ordenar por score otra vez por si acaso\n        order = np.argsort(-np.array(class_scores[c]))\n        tp = np.array(class_tp[c])[order]\n        fp = 1 - tp\n\n        tp_cum = np.cumsum(tp)\n        fp_cum = np.cumsum(fp)\n\n        rec = tp_cum / class_total_gt[c]\n        prec = tp_cum / np.maximum(tp_cum + fp_cum, np.finfo(np.float64).eps)\n\n        precisions.append(prec)\n        recalls.append(rec)\n\n    return precisions, recalls\n\nprecisions, recalls = compute_pr_curves(model, valid_dataset, device)\n\n# nombres de clases (mismo orden que catid_to_idx)\ninv_map = {v: k for k, v in catid_to_idx.items()}\ncategory_names = [str(inv_map[i]) for i in range(len(catid_to_idx))]\n\ndraw_precision_recall(precisions, recalls, category_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T21:46:05.283228Z","iopub.status.idle":"2025-12-08T21:46:05.283562Z","shell.execute_reply.started":"2025-12-08T21:46:05.283374Z","shell.execute_reply":"2025-12-08T21:46:05.283387Z"}},"outputs":[],"execution_count":null}]}