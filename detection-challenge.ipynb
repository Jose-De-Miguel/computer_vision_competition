{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":13943031,"datasetId":8886325,"databundleVersionId":14714254},{"sourceType":"modelInstanceVersion","sourceId":6125,"databundleVersionId":7429413,"modelInstanceId":4596},{"sourceType":"modelInstanceVersion","sourceId":6105,"databundleVersionId":7429373,"modelInstanceId":4648}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/josmdemiguel/detection-challenge?scriptVersionId=284444677\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## Assignment: Object detection\n- Alumno 1: José María De Miguel\n- Alumno 2: Miguel Ángel Rojo\n- Alumno 3: Sandra Millán\n\nThe goals of the assignment are:\n* Put into practice acquired knowledge to detect and recognize objects of interest within a satellite image.\n\nTo address this problem, you must choose one of the following options:\n*\tImplement a sliding window strategy to process the whole image, and then train a classifier that determines whether each window includes or not an object of interest. In this way, you can use previous image classification model to infer the object category.\n*\tBuild a single-stage object detection model (e.g., YOLO, SSD, RetinaNet, etc.).\n*\tBuild a two-stage object detection model (e.g., Faster R-CNN, R-FCN, etc.).\n\nFollow the link below to download the detection data set “xview_detection”: [https://drive.upm.es/s/P7nEf3Bygns7tbM](https://drive.upm.es/s/P7nEf3Bygns7tbM)","metadata":{"id":"zK490Ryuy-xN"}},{"cell_type":"code","source":"!pip install rasterio --quiet\n!pip install \"protobuf==3.20.*\" --quiet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T09:36:31.112537Z","iopub.execute_input":"2025-12-07T09:36:31.113181Z","iopub.status.idle":"2025-12-07T09:36:40.045932Z","shell.execute_reply.started":"2025-12-07T09:36:31.113157Z","shell.execute_reply":"2025-12-07T09:36:40.045065Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ntensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)\nprint(tf.config.list_physical_devices('GPU'))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T09:36:40.047756Z","iopub.execute_input":"2025-12-07T09:36:40.047996Z","iopub.status.idle":"2025-12-07T09:36:40.052881Z","shell.execute_reply.started":"2025-12-07T09:36:40.047972Z","shell.execute_reply":"2025-12-07T09:36:40.051958Z"}},"outputs":[{"name":"stdout","text":"2.18.0\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import uuid\nimport numpy as np\n\nclass GenericObject:\n    \"\"\"\n    Generic object data.\n    \"\"\"\n    def __init__(self):\n        self.id = uuid.uuid4()\n        self.bb = (-1, -1, -1, -1)\n        self.category= -1\n        self.score = -1\n\nclass GenericImage:\n    \"\"\"\n    Generic image data.\n    \"\"\"\n    def __init__(self, filename):\n        self.filename = filename\n        self.tile = np.array([-1, -1, -1, -1])  # (pt_x, pt_y, pt_x+width, pt_y+height)\n        self.objects = list([])\n\n    def add_object(self, obj: GenericObject):\n        self.objects.append(obj)","metadata":{"ExecuteTime":{"end_time":"2023-11-21T15:41:41.841195340Z","start_time":"2023-11-21T15:41:41.832078114Z"},"execution":{"iopub.status.busy":"2025-12-07T09:36:40.054387Z","iopub.execute_input":"2025-12-07T09:36:40.054577Z","iopub.status.idle":"2025-12-07T09:36:40.06751Z","shell.execute_reply.started":"2025-12-07T09:36:40.054561Z","shell.execute_reply":"2025-12-07T09:36:40.066965Z"},"id":"TGvvuEREy-xT","trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"categories = {0: 'Small car', 1: 'Bus', 2: 'Truck', 3: 'Building'}","metadata":{"execution":{"iopub.status.busy":"2025-12-07T09:36:40.069523Z","iopub.execute_input":"2025-12-07T09:36:40.069714Z","iopub.status.idle":"2025-12-07T09:36:40.081584Z","shell.execute_reply.started":"2025-12-07T09:36:40.0697Z","shell.execute_reply":"2025-12-07T09:36:40.08106Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import warnings\nimport rasterio\nimport numpy as np\n\ndef load_geoimage(filename):\n    warnings.filterwarnings('ignore', category=rasterio.errors.NotGeoreferencedWarning)\n    src_raster = rasterio.open('/kaggle/input/xview-detection/' + filename, 'r')\n    # RasterIO to OpenCV (see inconsistencies between libjpeg and libjpeg-turbo)\n    input_type = src_raster.profile['dtype']\n    input_channels = src_raster.count\n    img = np.zeros((src_raster.height, src_raster.width, src_raster.count), dtype=input_type)\n    for band in range(input_channels):\n        img[:, :, band] = src_raster.read(band+1)\n    return img","metadata":{"execution":{"iopub.status.busy":"2025-12-07T09:36:40.082161Z","iopub.execute_input":"2025-12-07T09:36:40.082338Z","iopub.status.idle":"2025-12-07T09:36:40.608809Z","shell.execute_reply.started":"2025-12-07T09:36:40.082324Z","shell.execute_reply":"2025-12-07T09:36:40.607998Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"#### Training\nDesign and train a detector to deal with the “xview_detection” perception task.","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import json\n\n# Load database\njson_file = '/kaggle/input/xview-detection/xview_det_train.json'\nwith open(json_file) as ifs:\n    json_data = json.load(ifs)\nifs.close()","metadata":{"ExecuteTime":{"end_time":"2023-11-21T15:48:22.102317718Z","start_time":"2023-11-21T15:48:20.911767630Z"},"collapsed":false,"execution":{"iopub.status.busy":"2025-12-07T09:36:40.609749Z","iopub.execute_input":"2025-12-07T09:36:40.610564Z","iopub.status.idle":"2025-12-07T09:36:43.296211Z","shell.execute_reply.started":"2025-12-07T09:36:40.610544Z","shell.execute_reply":"2025-12-07T09:36:43.29537Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import numpy as np\n\ncounts = dict.fromkeys(categories.values(), 0)\nanns = []\nfor json_img in json_data['images'].values():\n    image = GenericImage(json_img['filename'])\n    image.tile = np.array([0, 0, json_img['width'], json_img['height']])\n    for json_ann in [elem for elem in json_data['annotations'].values() if elem['image_id'] == json_img['image_id']]:\n        obj = GenericObject()\n        obj.id = json_ann['image_id']\n        obj.bb = (int(json_ann['bbox'][0]), int(json_ann['bbox'][1]), int(json_ann['bbox'][2]), int(json_ann['bbox'][3]))\n        obj.category = json_ann['category_id']\n        counts[obj.category] += 1\n        image.add_object(obj)\n    anns.append(image)\nprint(counts)","metadata":{"execution":{"iopub.status.busy":"2025-12-07T09:36:43.297108Z","iopub.execute_input":"2025-12-07T09:36:43.297355Z","iopub.status.idle":"2025-12-07T09:42:11.674081Z","shell.execute_reply.started":"2025-12-07T09:36:43.297332Z","shell.execute_reply":"2025-12-07T09:42:11.673292Z"},"trusted":true},"outputs":[{"name":"stdout","text":"{'Small car': 188300, 'Bus': 6269, 'Truck': 10600, 'Building': 275943}\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nanns_train, anns_valid = train_test_split(anns, test_size=0.1, random_state=1, shuffle=True)\nprint('Number of training images: ' + str(len(anns_train)))\nprint('Number of validation images: ' + str(len(anns_valid)))","metadata":{"execution":{"iopub.status.busy":"2025-12-07T09:42:11.67508Z","iopub.execute_input":"2025-12-07T09:42:11.675441Z","iopub.status.idle":"2025-12-07T09:42:11.875616Z","shell.execute_reply.started":"2025-12-07T09:42:11.675416Z","shell.execute_reply":"2025-12-07T09:42:11.875002Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Number of training images: 6845\nNumber of validation images: 761\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import keras_cv\nimport inspect\n\nprint(keras_cv.__version__)\nprint(dir(keras_cv.models))  # ver qué modelos hay realmente","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T09:42:11.876276Z","iopub.execute_input":"2025-12-07T09:42:11.876451Z","iopub.status.idle":"2025-12-07T09:42:20.221437Z","shell.execute_reply.started":"2025-12-07T09:42:11.876437Z","shell.execute_reply":"2025-12-07T09:42:20.220801Z"}},"outputs":[{"name":"stdout","text":"0.9.0\n['BASNet', 'Backbone', 'CLIP', 'CSPDarkNetBackbone', 'CSPDarkNetLBackbone', 'CSPDarkNetMBackbone', 'CSPDarkNetSBackbone', 'CSPDarkNetTinyBackbone', 'CSPDarkNetXLBackbone', 'CenterPillarBackbone', 'DeepLabV3Plus', 'DenseNet121Backbone', 'DenseNet169Backbone', 'DenseNet201Backbone', 'DenseNetBackbone', 'EfficientNetLiteB0Backbone', 'EfficientNetLiteB1Backbone', 'EfficientNetLiteB2Backbone', 'EfficientNetLiteB3Backbone', 'EfficientNetLiteB4Backbone', 'EfficientNetLiteBackbone', 'EfficientNetV1B0Backbone', 'EfficientNetV1B1Backbone', 'EfficientNetV1B2Backbone', 'EfficientNetV1B3Backbone', 'EfficientNetV1B4Backbone', 'EfficientNetV1B5Backbone', 'EfficientNetV1B6Backbone', 'EfficientNetV1B7Backbone', 'EfficientNetV1Backbone', 'EfficientNetV2B0Backbone', 'EfficientNetV2B1Backbone', 'EfficientNetV2B2Backbone', 'EfficientNetV2B3Backbone', 'EfficientNetV2Backbone', 'EfficientNetV2LBackbone', 'EfficientNetV2MBackbone', 'EfficientNetV2SBackbone', 'ImageClassifier', 'MiTB0Backbone', 'MiTB1Backbone', 'MiTB2Backbone', 'MiTB3Backbone', 'MiTB4Backbone', 'MiTB5Backbone', 'MiTBackbone', 'MobileNetV3Backbone', 'MobileNetV3LargeBackbone', 'MobileNetV3SmallBackbone', 'MultiHeadCenterPillar', 'ResNet101Backbone', 'ResNet101V2Backbone', 'ResNet152Backbone', 'ResNet152V2Backbone', 'ResNet18Backbone', 'ResNet18V2Backbone', 'ResNet34Backbone', 'ResNet34V2Backbone', 'ResNet50Backbone', 'ResNet50V2Backbone', 'ResNetBackbone', 'ResNetV2Backbone', 'RetinaNet', 'SAMMaskDecoder', 'SAMPromptEncoder', 'SegFormer', 'SegFormerB0', 'SegFormerB1', 'SegFormerB2', 'SegFormerB3', 'SegFormerB4', 'SegFormerB5', 'SegmentAnythingModel', 'StableDiffusion', 'StableDiffusionV2', 'Task', 'TwoWayTransformer', 'VGG16Backbone', 'ViTDetBBackbone', 'ViTDetBackbone', 'ViTDetHBackbone', 'ViTDetLBackbone', 'VideoClassifier', 'VideoSwinBBackbone', 'VideoSwinBackbone', 'VideoSwinSBackbone', 'VideoSwinTBackbone', 'YOLOV8Backbone', 'YOLOV8Detector', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'classification', 'feature_extractor', 'object_detection', 'retinanet', 'segmentation', 'stable_diffusion', 'yolov8']\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import keras_cv\n\nprint(\"ResNetBackbone presets:\", getattr(keras_cv.models.ResNetBackbone, \"presets\", {}).keys())\nprint(\"ResNet50Backbone presets:\", getattr(keras_cv.models.ResNet50Backbone, \"presets\", {}).keys())\nprint(\"EfficientNetV2Backbone presets:\", getattr(keras_cv.models.EfficientNetV2Backbone, \"presets\", {}).keys())\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T09:42:20.223706Z","iopub.execute_input":"2025-12-07T09:42:20.223934Z","iopub.status.idle":"2025-12-07T09:42:20.230106Z","shell.execute_reply.started":"2025-12-07T09:42:20.223916Z","shell.execute_reply":"2025-12-07T09:42:20.229291Z"}},"outputs":[{"name":"stdout","text":"ResNetBackbone presets: dict_keys(['resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152', 'resnet50_imagenet'])\nResNet50Backbone presets: dict_keys(['resnet50_imagenet'])\nEfficientNetV2Backbone presets: dict_keys(['efficientnetv2_s', 'efficientnetv2_m', 'efficientnetv2_l', 'efficientnetv2_b0', 'efficientnetv2_b1', 'efficientnetv2_b2', 'efficientnetv2_b3', 'efficientnetv2_s_imagenet', 'efficientnetv2_b0_imagenet', 'efficientnetv2_b1_imagenet', 'efficientnetv2_b2_imagenet'])\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"IMAGE_SIZE = 640\n\ndef image_generator(filename, tile, bboxes, categories):\n    def load_sample(filepath):\n        image = load_geoimage(filepath.numpy().decode('utf-8'))\n        return tf.cast(image, tf.uint8)\n\n    # 1. Cargar con py_function\n    img = tf.squeeze(\n        tf.py_function(func=load_sample, inp=[filename], Tout=[tf.uint8]),\n        axis=0\n    )\n    # 2. Fijar shape de canal\n    img.set_shape([None, None, 3])\n\n    # 3. Ajustar a 640x640 (tú ahora haces pad_to_bounding_box)\n    img_roi = tf.image.pad_to_bounding_box(img, 0, 0, IMAGE_SIZE, IMAGE_SIZE)\n    img_roi.set_shape([IMAGE_SIZE, IMAGE_SIZE, 3])\n\n    # 4. Normalizar\n    img_roi = tf.cast(img_roi, tf.float32) / 255.0\n\n    # 5. Devolver SIEMPRE dict con 'images' y 'bounding_boxes'\n    return {\n        \"images\": img_roi,\n        \"bounding_boxes\": {\n            \"boxes\": bboxes,\n            \"classes\": categories,\n        },\n    }\n\n\n\ndef ragged_to_dense(inputs):\n    from keras_cv import bounding_box\n    return {\n        'images': inputs['images'],  # <- ya es un tensor normal, no .to_tensor()\n        'bounding_boxes': bounding_box.to_dense(\n            inputs['bounding_boxes'],\n            max_boxes=2000\n        ),\n    }\n\n\ndef dict_to_tuple(inputs):\n    return inputs['images'], inputs['bounding_boxes']\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T09:42:20.231036Z","iopub.execute_input":"2025-12-07T09:42:20.231442Z","iopub.status.idle":"2025-12-07T09:42:20.263218Z","shell.execute_reply.started":"2025-12-07T09:42:20.23141Z","shell.execute_reply":"2025-12-07T09:42:20.262658Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Generate the list of objects from annotations\nfilenames_train, tiles_train, bboxes_train, categories_train = zip(*list(map(lambda img_ann: (img_ann.filename, list(img_ann.tile), list([list(obj_ann.bb) for obj_ann in img_ann.objects]), list([list(categories.keys())[list(categories.values()).index(obj_ann.category)] for obj_ann in img_ann.objects])), anns_train)))\nfilenames_valid, tiles_valid, bboxes_valid, categories_valid = zip(*list(map(lambda img_ann: (img_ann.filename, list(img_ann.tile), list([list(obj_ann.bb) for obj_ann in img_ann.objects]), list([list(categories.keys())[list(categories.values()).index(obj_ann.category)] for obj_ann in img_ann.objects])), anns_valid)))\nds_train = tf.data.Dataset.from_tensor_slices((tf.cast(filenames_train, tf.string), tf.cast(tiles_train, tf.int32), tf.cast(tf.ragged.constant(bboxes_train), tf.float32).to_tensor(), tf.cast(tf.ragged.constant(categories_train), tf.float32).to_tensor()))\nds_valid = tf.data.Dataset.from_tensor_slices((tf.cast(filenames_valid, tf.string), tf.cast(tiles_valid, tf.int32), tf.cast(tf.ragged.constant(bboxes_valid), tf.float32).to_tensor(), tf.cast(tf.ragged.constant(categories_valid), tf.float32).to_tensor()))\n# ds_train / ds_valid creados como tú\n\nds_train = ds_train.map(image_generator, num_parallel_calls=tf.data.AUTOTUNE)\nds_valid = ds_valid.map(image_generator, num_parallel_calls=tf.data.AUTOTUNE)\n\nbatch_size = 4\n\n# Mezclar y hacer batch\nds_train = ds_train.shuffle(batch_size * 5)\nds_train = ds_train.ragged_batch(batch_size=batch_size, drop_remainder=True)\nds_valid = ds_valid.ragged_batch(batch_size=batch_size, drop_remainder=True)\n\n# Data augmentation que acepta dict con images+bounding_boxes\ndata_augmentation = tf.keras.Sequential(\n    layers=[\n        keras_cv.layers.RandomFlip(\n            mode='horizontal_and_vertical',\n            bounding_box_format='xyxy',\n        ),\n        keras_cv.layers.RandomShear(\n            x_factor=0.2,\n            y_factor=0.2,\n            bounding_box_format='xyxy',\n        ),\n        keras_cv.layers.RandomColorDegeneration(factor=0.5),\n    ]\n)\n\nds_train = ds_train.map(data_augmentation, num_parallel_calls=tf.data.AUTOTUNE)\n\n# Densificar SOLO las bounding boxes\nds_train = ds_train.map(ragged_to_dense, num_parallel_calls=tf.data.AUTOTUNE)\nds_valid = ds_valid.map(ragged_to_dense, num_parallel_calls=tf.data.AUTOTUNE)\n\n# NADA de dict_to_tuple aquí\nds_train = ds_train.prefetch(tf.data.AUTOTUNE)\nds_valid = ds_valid.prefetch(tf.data.AUTOTUNE)\n","metadata":{"execution":{"iopub.status.busy":"2025-12-07T09:46:56.773923Z","iopub.execute_input":"2025-12-07T09:46:56.774285Z","iopub.status.idle":"2025-12-07T09:47:01.991173Z","shell.execute_reply.started":"2025-12-07T09:46:56.774263Z","shell.execute_reply":"2025-12-07T09:47:01.990551Z"},"trusted":true},"outputs":[],"execution_count":20},{"cell_type":"code","source":"ds_train.take(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T09:47:01.992125Z","iopub.execute_input":"2025-12-07T09:47:01.992406Z","iopub.status.idle":"2025-12-07T09:47:01.99789Z","shell.execute_reply.started":"2025-12-07T09:47:01.992386Z","shell.execute_reply":"2025-12-07T09:47:01.99722Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"<_TakeDataset element_spec={'images': TensorSpec(shape=(4, 640, 640, 3), dtype=tf.float32, name=None), 'bounding_boxes': {'boxes': TensorSpec(shape=(4, 2000, 4), dtype=tf.float32, name=None), 'classes': TensorSpec(shape=(4, 2000), dtype=tf.float32, name=None)}}>"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"NUM_CLASSES = len(categories)\nIMAGE_SIZE = 640\nNUM_EPOCHS = 40","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T09:42:25.99065Z","iopub.execute_input":"2025-12-07T09:42:25.990873Z","iopub.status.idle":"2025-12-07T09:42:26.0019Z","shell.execute_reply.started":"2025-12-07T09:42:25.990849Z","shell.execute_reply":"2025-12-07T09:42:26.001382Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import keras_cv\nfrom tensorflow import keras\n\n\n\n# (Opcional) NMS extra si quieres post-filtrar resultados\nprediction_decoder = keras_cv.layers.NonMaxSuppression(\n    bounding_box_format='xyxy',\n    from_logits=False,\n    confidence_threshold=0.2,\n    iou_threshold=0.7,\n)\n\n\nbackbone = keras_cv.models.ResNetBackbone.from_preset(\n    \"resnet50_imagenet\",\n    input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n    include_rescaling=True,\n)\n\nmodel = keras_cv.models.RetinaNet(\n    num_classes=NUM_CLASSES,\n    bounding_box_format=\"xyxy\",  \n    backbone=backbone,\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2025-12-07T09:42:26.0026Z","iopub.execute_input":"2025-12-07T09:42:26.002797Z","iopub.status.idle":"2025-12-07T09:42:31.608788Z","shell.execute_reply.started":"2025-12-07T09:42:26.002776Z","shell.execute_reply":"2025-12-07T09:42:31.608215Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"code","source":"from tensorflow import keras\nimport keras_cv\n\nsteps_per_epoch = len(ds_train)  # pon aquí tu valor real\nepochs = NUM_EPOCHS\n\nbase_lr = 1e-4\n\nlr_schedule = keras.optimizers.schedules.CosineDecay(\n    initial_learning_rate=base_lr,\n    decay_steps=steps_per_epoch * epochs,\n    alpha=0.1,  # LR final = 10% de la inicial\n)\n\nopt = keras.optimizers.AdamW(\n    learning_rate=lr_schedule,\n    weight_decay=5e-4,\n    global_clipnorm=10.0,\n)\n\nclassification_loss = keras_cv.losses.FocalLoss(\n    from_logits=True,\n    alpha=0.25,\n    gamma=2.0,\n)\n\nbox_loss = keras_cv.losses.SmoothL1Loss(\n    l1_cutoff=0.1,                 # <- este es el parámetro correcto en 0.9.0\n    reduction=\"sum_over_batch_size\",\n)\n\n\nmodel.compile(\n    optimizer=opt,\n    classification_loss=classification_loss,\n    box_loss=box_loss,\n    jit_compile=False,  # si te va estable, puedes probar True para más velocidad\n)\n","metadata":{"execution":{"iopub.status.busy":"2025-12-07T09:42:31.609828Z","iopub.execute_input":"2025-12-07T09:42:31.610055Z","iopub.status.idle":"2025-12-07T09:42:31.622533Z","shell.execute_reply.started":"2025-12-07T09:42:31.610039Z","shell.execute_reply":"2025-12-07T09:42:31.621919Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from tensorflow.keras.callbacks import TerminateOnNaN, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\n# Callbacks\nmodel_checkpoint = ModelCheckpoint('/kaggle/working/R-CNN-training1.keras', monitor='val_loss', verbose=1, save_best_only=True)\n# no usar con cosine decay: reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1, patience=10, verbose=1)\nearly_stop = EarlyStopping('val_loss', patience=10, verbose=1)\nterminate = TerminateOnNaN()\ncallbacks = [model_checkpoint, early_stop, terminate]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T09:42:31.623256Z","iopub.execute_input":"2025-12-07T09:42:31.623545Z","iopub.status.idle":"2025-12-07T09:42:31.627948Z","shell.execute_reply.started":"2025-12-07T09:42:31.623521Z","shell.execute_reply":"2025-12-07T09:42:31.627202Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import numpy as np\n\nprint('Training model')\nepochs = NUM_EPOCHS\ntrain_steps, valid_steps = len(ds_train), len(ds_valid)\n\n\nh = model.fit(\n    ds_train,\n    validation_data=ds_valid,\n    epochs=NUM_EPOCHS,\n    #callbacks=callbacks,\n    verbose=1,\n)\n# Best validation model\nbest_idx = int(np.argmin(h.history['val_loss']))\nbest_value = np.min(h.history['val_loss'])\nprint('Best validation model: epoch ' + str(best_idx+1), ' - val_loss ' + str(best_value))","metadata":{"execution":{"iopub.status.busy":"2025-12-07T09:51:43.43667Z","iopub.execute_input":"2025-12-07T09:51:43.437268Z","iopub.status.idle":"2025-12-07T09:51:46.453903Z","shell.execute_reply.started":"2025-12-07T09:51:43.437245Z","shell.execute_reply":"2025-12-07T09:51:46.452712Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Training model\nEpoch 1/40\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/2244054983.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m h = model.fit(\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mds_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_cv/src/models/object_detection/retinanet/retinanet.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpack_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Reshape.call().\n\n\u001b[1mCannot reshape a tensor with 14400 elements to shape [4,3600,4] (57600 elements) for '{{node retina_net_1/reshape_7_1/Reshape}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](retina_net_1/prediction_head_3/conv2d_11_1/BiasAdd, retina_net_1/reshape_7_1/Reshape/shape)' with input shapes: [4,10,10,36], [3] and with input tensors computed as partial shapes: input[1] = [4,3600,4].\u001b[0m\n\nArguments received by Reshape.call():\n  • inputs=tf.Tensor(shape=(4, 10, 10, 36), dtype=float32)"],"ename":"ValueError","evalue":"Exception encountered when calling Reshape.call().\n\n\u001b[1mCannot reshape a tensor with 14400 elements to shape [4,3600,4] (57600 elements) for '{{node retina_net_1/reshape_7_1/Reshape}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](retina_net_1/prediction_head_3/conv2d_11_1/BiasAdd, retina_net_1/reshape_7_1/Reshape/shape)' with input shapes: [4,10,10,36], [3] and with input tensors computed as partial shapes: input[1] = [4,3600,4].\u001b[0m\n\nArguments received by Reshape.call():\n  • inputs=tf.Tensor(shape=(4, 10, 10, 36), dtype=float32)","output_type":"error"}],"execution_count":23},{"cell_type":"code","source":"with open(\"RetinaNet-1.json\", \"w\") as f:\n    json.dump(h.history, f)\n\nmodel.save(\"RetinaNet-1.keras\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T09:51:27.430735Z","iopub.execute_input":"2025-12-07T09:51:27.431521Z","iopub.status.idle":"2025-12-07T09:51:27.452813Z","shell.execute_reply.started":"2025-12-07T09:51:27.431489Z","shell.execute_reply":"2025-12-07T09:51:27.451974Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/4085810495.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RetinaNet-1.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RetinaNet-1.keras\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'h' is not defined"],"ename":"NameError","evalue":"name 'h' is not defined","output_type":"error"}],"execution_count":22},{"cell_type":"markdown","source":"#### Validation\nCompute validation metrics.","metadata":{"collapsed":false,"id":"pyX0YILvy-xa","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.colors as col\nimport numpy as np\n%matplotlib inline\n\ndef area_intersection(boxes, box):\n    xmin = np.maximum(np.min(boxes[:, 0::2], axis=1), np.min(box[0::2]))\n    ymin = np.maximum(np.min(boxes[:, 1::2], axis=1), np.min(box[1::2]))\n    xmax = np.minimum(np.max(boxes[:, 0::2], axis=1), np.max(box[0::2]))\n    ymax = np.minimum(np.max(boxes[:, 1::2], axis=1), np.max(box[1::2]))\n    w = np.maximum(xmax - xmin + 1.0, 0.0)\n    h = np.maximum(ymax - ymin + 1.0, 0.0)\n    return w * h\n\ndef area_union(boxes, box):\n    area_anns = (np.max(box[0::2])-np.min(box[0::2])+1.0) * (np.max(box[1::2])-np.min(box[1::2])+1.0)\n    area_pred = (np.max(boxes[:, 0::2], axis=1)-np.min(boxes[:, 0::2], axis=1)+1.0) * (np.max(boxes[:, 1::2], axis=1)-np.min(boxes[:, 1::2], axis=1)+1.0)\n    return area_anns + area_pred - area_intersection(boxes, box)\n\ndef calc_iou(boxes, box):\n    iou = area_intersection(boxes, box) / area_union(boxes, box)\n    max_value = np.max(iou)\n    max_index = np.argmax(iou)\n    return max_value, max_index\n\ndef calc_ap(rec, prec):\n    # First append sentinel values at the end\n    mrec = np.concatenate(([0.0], rec, [1.0]))\n    mpre = np.concatenate(([0.0], prec, [0.0]))\n    # Compute the precision envelope\n    for i in range(mpre.size-1, 0, -1):\n        mpre[i-1] = np.maximum(mpre[i-1], mpre[i])\n    # To calculate area under PR curve, look for points where X axis (recall) changes value\n    i = np.where(mrec[1:] != mrec[:-1])[0]\n    # and sum (\\Delta recall) * prec\n    ap = np.sum((mrec[i+1] - mrec[i]) * mpre[i+1])\n    return ap\n\ndef draw_confusion_matrix(cm, categories):\n    # Draw confusion matrix\n    fig = plt.figure(figsize=[6.4*pow(len(categories), 0.5), 4.8*pow(len(categories), 0.5)])\n    ax = fig.add_subplot(111)\n    cm = cm.astype('float') / np.maximum(cm.sum(axis=1)[:, np.newaxis], np.finfo(np.float64).eps)\n    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.get_cmap('Blues'))\n    ax.figure.colorbar(im, ax=ax)\n    ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]), xticklabels=categories, yticklabels=categories, ylabel='Annotation', xlabel='Prediction')\n    # Rotate the tick labels and set their alignment\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n    # Loop over data dimensions and create text annotations\n    thresh = cm.max() / 2.0\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], '.2f'), ha=\"center\", va=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=int(20-pow(len(categories), 0.5)))\n    fig.tight_layout()\n    plt.show()\n\ndef draw_precision_recall(precisions, recalls, categories):\n    # Draw precision-recall curves for each category\n    fig = plt.figure(figsize=[6.4*pow(len(categories), 0.5), 4.8*pow(len(categories), 0.5)])\n    ax = fig.add_subplot(111)\n    plt.axis([0, 1, 0, 1])\n    c_dark = list(filter(lambda x: x.startswith('dark'), col.cnames.keys()))\n    aps = []\n    # Compare categories for a specific algorithm\n    for idx in range(len(categories)):\n        plt.plot(recalls[idx], precisions[idx], color=c_dark[idx], label=categories[idx], linewidth=4.0)\n        aps.append(calc_ap(recalls[idx], precisions[idx]))\n    handles, labels = ax.get_legend_handles_labels()\n    labels = [str(val + ' [' + \"{:.3f}\".format(aps[idx]) + ']') for idx, val in enumerate(labels)]\n    handles = [h for (ap, h) in sorted(zip(aps, handles), key=lambda x: x[0], reverse=True)]\n    labels = [l for (ap, l) in sorted(zip(aps, labels), key=lambda x: x[0], reverse=True)]\n    leg = plt.legend(handles, labels, loc='upper right')\n    leg.set_zorder(100)\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.grid(\"on\", linestyle=\"--\", linewidth=2.0)\n    fig.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2025-12-07T09:42:34.596106Z","iopub.status.idle":"2025-12-07T09:42:34.59657Z","shell.execute_reply.started":"2025-12-07T09:42:34.596448Z","shell.execute_reply":"2025-12-07T09:42:34.59646Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom tqdm import tqdm\n\n#model.load_weights('model.keras')\n# Generate the list of objects from annotations\nds_valid = tf.data.Dataset.from_tensor_slices((tf.cast(filenames_valid, tf.string), tf.cast(tiles_valid, tf.int32), tf.cast(tf.ragged.constant(bboxes_valid), tf.float32).to_tensor(), tf.cast(tf.ragged.constant(categories_valid), tf.float32).to_tensor()))\nds_valid = ds_valid.map(image_generator, num_parallel_calls=tf.data.AUTOTUNE).cache()\nds_valid = ds_valid.batch(batch_size=1)\nds_valid = ds_valid.prefetch(tf.data.AUTOTUNE)\n# Process each tile sequentially\niterator = iter(ds_valid)\nannotations, predictions = {}, {}\nfor ann in tqdm(anns_valid):\n    # Save annotations\n    annotations.setdefault(ann.filename, {})\n    predictions.setdefault(ann.filename, {})\n    for obj in ann.objects:\n        annotations[ann.filename].setdefault(obj.category, {'bbox': []})\n        annotations[ann.filename][obj.category]['bbox'].append(obj.bb)\n    # Save prediction\n    image, _ = next(iterator)\n    y_pred = model.predict(image, verbose=0)\n    for i in range(np.squeeze(y_pred['num_detections'])):\n        obj = GenericObject()\n        bbox = np.squeeze(y_pred['boxes'])[i]\n        obj.bb = (bbox[0], bbox[1], bbox[2], bbox[3])\n        obj.category = categories[np.squeeze(y_pred['classes'])[i]]\n        obj.score = np.squeeze(y_pred['confidence'])[i]\n        predictions[ann.filename].setdefault(obj.category, {'bbox': [], 'confidence': []})\n        predictions[ann.filename][obj.category]['bbox'].append(obj.bb)\n        predictions[ann.filename][obj.category]['confidence'].append(obj.score)  # sort detections by confidence","metadata":{"collapsed":false,"execution":{"iopub.status.busy":"2025-12-07T09:42:34.59739Z","iopub.status.idle":"2025-12-07T09:42:34.597689Z","shell.execute_reply.started":"2025-12-07T09:42:34.597533Z","shell.execute_reply":"2025-12-07T09:42:34.597548Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"threshold = 0.5\ndefault_cls = 'BACKGROUND'\ny_true, y_pred = [], []  # confusion matrix\ntps, confidences = dict(), dict()  # draw precision-recall curves for each category\nfor cls in categories.values():\n    # Compute TP, FP and FN for each image\n    tps[cls], confidences[cls] = [], []\n    for f in predictions:\n        # Sort 'cls' predictions by confidence for each file\n        pred_boxes, pred_confidences = [], []\n        if cls in predictions[f].keys():\n            for idx in range(len(predictions[f][cls]['bbox'])):\n                pred_boxes.append(predictions[f][cls]['bbox'][idx])\n                pred_confidences.append(predictions[f][cls]['confidence'][idx])\n            sorted_ind = np.argsort(-np.array(pred_confidences))\n            pred_boxes = np.array(pred_boxes)[sorted_ind, :]\n        pred_boxes = np.array(pred_boxes).astype(float)\n        # Define 'cls' annotations for each file\n        anno_boxes = []\n        if cls in annotations[f].keys():\n            anno_boxes = annotations[f][cls]['bbox']\n        anno_boxes = np.array(anno_boxes).astype(float)\n        # Define horizontal or oriented bounding boxes\n        anno_indices = list(range(len(anno_boxes)))\n        # Compare a single prediction 'pred_box' with all annotations 'anno_boxes'\n        for pred_idx, pred_box in enumerate(pred_boxes):\n            # A prediction is correct if its IoU with the ground truth is above the threshold\n            iou_value, ann_index = calc_iou(anno_boxes, pred_box) if len(anno_boxes) > 0 else (-1, -1)\n            if iou_value > threshold and ann_index in anno_indices:\n                # TP\n                anno_indices.remove(int(ann_index))\n                tps[cls] += [1.0]\n                y_true += [cls]\n            else:\n                # FP\n                tps[cls] += [0.0]\n                y_true += [default_cls]\n            y_pred += [cls]\n            confidences[cls] += [pred_confidences[pred_idx]]\n        # FN\n        y_true += [cls] * len(anno_indices)\n        y_pred += [default_cls] * len(anno_indices)\ny_true, y_pred = np.array(y_true), np.array(y_pred)","metadata":{"execution":{"iopub.status.busy":"2025-12-07T09:42:34.598686Z","iopub.status.idle":"2025-12-07T09:42:34.598982Z","shell.execute_reply.started":"2025-12-07T09:42:34.598838Z","shell.execute_reply":"2025-12-07T09:42:34.598853Z"},"id":"QsA9Dx57y-xf","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score\n\n# Compute AP metric\nprecision_list, recall_list, ap_list = [], [], []\nfor cls in categories.values():\n    sorted_ind = np.argsort(-np.array(confidences[cls]))\n    tp = np.cumsum(np.array(tps[cls])[sorted_ind], dtype=float)\n    recall = np.array([0.0]) if len(tp) == 0 else tp / np.maximum(np.sum(y_true == cls), np.finfo(np.float64).eps)\n    precision = np.array([0.0]) if len(tp) == 0 else tp / np.maximum(list(range(1, np.sum(y_pred == cls)+1)), np.finfo(np.float64).eps)\n    ap = calc_ap(recall, precision)\n    print('> %s: Recall: %.3f%% Precision: %.3f%% AP: %.3f%%' % (cls, recall[-1]*100, precision[-1]*100, ap*100))\n    precision_list.append(precision)\n    recall_list.append(recall)\n    ap_list.append(ap)\nmean_ap = np.mean(ap_list)\nprint('mAccuracy: %.3f%%' % (accuracy_score(y_true, y_pred)*100))\nprint('mRecall: %.3f%%' % (recall_score(y_true, y_pred, average='macro', zero_division=1)*100))\nprint('mPrecision: %.3f%%' % (precision_score(y_true, y_pred, average='macro', zero_division=1)*100))\nprint('mAP: %.3f%%' % (mean_ap*100))","metadata":{"execution":{"iopub.status.busy":"2025-12-07T09:42:34.599951Z","iopub.status.idle":"2025-12-07T09:42:34.600198Z","shell.execute_reply.started":"2025-12-07T09:42:34.600097Z","shell.execute_reply":"2025-12-07T09:42:34.600107Z"},"id":"6vXYDcQ0y-xh","outputId":"2b814533-4932-435c-c595-2d03ff96af52","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"names = list(categories.values()).copy()\nnames.insert(0, default_cls)\ncm = confusion_matrix(y_true, y_pred, labels=names)\nprint('Confusion matrix:')\nprint(cm)\ndraw_confusion_matrix(cm, names)\ndraw_precision_recall(precision_list, recall_list, categories)","metadata":{"execution":{"iopub.status.busy":"2025-12-07T09:42:34.601454Z","iopub.status.idle":"2025-12-07T09:42:34.601756Z","shell.execute_reply.started":"2025-12-07T09:42:34.601601Z","shell.execute_reply":"2025-12-07T09:42:34.601615Z"},"id":"P30Z5qLGy-xi","outputId":"62bf9fc1-bdfe-48a8-8060-d52fe3a81dab","trusted":true},"outputs":[],"execution_count":null}]}