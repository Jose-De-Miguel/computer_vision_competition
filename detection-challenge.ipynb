{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13943031,"sourceType":"datasetVersion","datasetId":8886325},{"sourceId":6105,"sourceType":"modelInstanceVersion","modelInstanceId":4648,"modelId":2804}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/josmdemiguel/detection-challenge?scriptVersionId=283433896\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## Assignment: Object detection\n- Alumno 1: José María De Miguel\n- Alumno 2: Miguel Ángel Rojo\n- Alumno 3: Sandra Millán\n\nThe goals of the assignment are:\n* Put into practice acquired knowledge to detect and recognize objects of interest within a satellite image.\n\nTo address this problem, you must choose one of the following options:\n*\tImplement a sliding window strategy to process the whole image, and then train a classifier that determines whether each window includes or not an object of interest. In this way, you can use previous image classification model to infer the object category.\n*\tBuild a single-stage object detection model (e.g., YOLO, SSD, RetinaNet, etc.).\n*\tBuild a two-stage object detection model (e.g., Faster R-CNN, R-FCN, etc.).\n\nFollow the link below to download the detection data set “xview_detection”: [https://drive.upm.es/s/P7nEf3Bygns7tbM](https://drive.upm.es/s/P7nEf3Bygns7tbM)","metadata":{"id":"zK490Ryuy-xN"}},{"cell_type":"code","source":"!pip install rasterio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T09:30:58.091563Z","iopub.execute_input":"2025-12-01T09:30:58.092085Z","iopub.status.idle":"2025-12-01T09:31:02.925973Z","shell.execute_reply.started":"2025-12-01T09:30:58.092059Z","shell.execute_reply":"2025-12-01T09:31:02.925054Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)\nprint(tf.config.list_physical_devices('GPU'))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T09:16:27.863472Z","iopub.execute_input":"2025-12-01T09:16:27.863757Z","iopub.status.idle":"2025-12-01T09:16:27.86858Z","shell.execute_reply.started":"2025-12-01T09:16:27.863738Z","shell.execute_reply":"2025-12-01T09:16:27.867784Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import uuid\nimport numpy as np\n\nclass GenericObject:\n    \"\"\"\n    Generic object data.\n    \"\"\"\n    def __init__(self):\n        self.id = uuid.uuid4()\n        self.bb = (-1, -1, -1, -1)\n        self.category= -1\n        self.score = -1\n\nclass GenericImage:\n    \"\"\"\n    Generic image data.\n    \"\"\"\n    def __init__(self, filename):\n        self.filename = filename\n        self.tile = np.array([-1, -1, -1, -1])  # (pt_x, pt_y, pt_x+width, pt_y+height)\n        self.objects = list([])\n\n    def add_object(self, obj: GenericObject):\n        self.objects.append(obj)","metadata":{"ExecuteTime":{"end_time":"2023-11-21T15:41:41.841195340Z","start_time":"2023-11-21T15:41:41.832078114Z"},"execution":{"iopub.status.busy":"2025-12-01T09:16:36.696478Z","iopub.execute_input":"2025-12-01T09:16:36.696822Z","iopub.status.idle":"2025-12-01T09:16:36.702336Z","shell.execute_reply.started":"2025-12-01T09:16:36.696802Z","shell.execute_reply":"2025-12-01T09:16:36.701652Z"},"id":"TGvvuEREy-xT","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"categories = {0: 'Small car', 1: 'Bus', 2: 'Truck', 3: 'Building'}","metadata":{"execution":{"iopub.status.busy":"2025-12-01T09:16:38.989109Z","iopub.execute_input":"2025-12-01T09:16:38.990003Z","iopub.status.idle":"2025-12-01T09:16:38.993627Z","shell.execute_reply.started":"2025-12-01T09:16:38.989968Z","shell.execute_reply":"2025-12-01T09:16:38.992915Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nimport rasterio\nimport numpy as np\n\ndef load_geoimage(filename):\n    warnings.filterwarnings('ignore', category=rasterio.errors.NotGeoreferencedWarning)\n    src_raster = rasterio.open('/kaggle/input/xview-detection/' + filename, 'r')\n    # RasterIO to OpenCV (see inconsistencies between libjpeg and libjpeg-turbo)\n    input_type = src_raster.profile['dtype']\n    input_channels = src_raster.count\n    img = np.zeros((src_raster.height, src_raster.width, src_raster.count), dtype=input_type)\n    for band in range(input_channels):\n        img[:, :, band] = src_raster.read(band+1)\n    return img","metadata":{"execution":{"iopub.status.busy":"2025-12-01T09:31:27.020228Z","iopub.execute_input":"2025-12-01T09:31:27.020816Z","iopub.status.idle":"2025-12-01T09:31:27.405645Z","shell.execute_reply.started":"2025-12-01T09:31:27.02078Z","shell.execute_reply":"2025-12-01T09:31:27.40503Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Training\nDesign and train a detector to deal with the “xview_detection” perception task.","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import json\n\n# Load database\njson_file = '/kaggle/input/xview-detection/xview_det_train.json'\nwith open(json_file) as ifs:\n    json_data = json.load(ifs)\nifs.close()","metadata":{"ExecuteTime":{"end_time":"2023-11-21T15:48:22.102317718Z","start_time":"2023-11-21T15:48:20.911767630Z"},"collapsed":false,"execution":{"iopub.status.busy":"2025-12-01T09:31:56.309252Z","iopub.execute_input":"2025-12-01T09:31:56.309545Z","iopub.status.idle":"2025-12-01T09:31:59.118028Z","shell.execute_reply.started":"2025-12-01T09:31:56.309525Z","shell.execute_reply":"2025-12-01T09:31:59.117244Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\ncounts = dict.fromkeys(categories.values(), 0)\nanns = []\nfor json_img in json_data['images'].values():\n    image = GenericImage(json_img['filename'])\n    image.tile = np.array([0, 0, json_img['width'], json_img['height']])\n    for json_ann in [elem for elem in json_data['annotations'].values() if elem['image_id'] == json_img['image_id']]:\n        obj = GenericObject()\n        obj.id = json_ann['image_id']\n        obj.bb = (int(json_ann['bbox'][0]), int(json_ann['bbox'][1]), int(json_ann['bbox'][2]), int(json_ann['bbox'][3]))\n        obj.category = json_ann['category_id']\n        counts[obj.category] += 1\n        image.add_object(obj)\n    anns.append(image)\nprint(counts)","metadata":{"execution":{"iopub.status.busy":"2025-12-01T09:31:59.119112Z","iopub.execute_input":"2025-12-01T09:31:59.119423Z","iopub.status.idle":"2025-12-01T09:37:29.585607Z","shell.execute_reply.started":"2025-12-01T09:31:59.119393Z","shell.execute_reply":"2025-12-01T09:37:29.584842Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nanns_train, anns_valid = train_test_split(anns, test_size=0.1, random_state=1, shuffle=True)\nprint('Number of training images: ' + str(len(anns_train)))\nprint('Number of validation images: ' + str(len(anns_valid)))","metadata":{"execution":{"iopub.status.busy":"2025-12-01T09:37:29.586862Z","iopub.execute_input":"2025-12-01T09:37:29.58707Z","iopub.status.idle":"2025-12-01T09:37:29.684864Z","shell.execute_reply.started":"2025-12-01T09:37:29.587054Z","shell.execute_reply":"2025-12-01T09:37:29.684094Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load architecture\nimport keras_cv\n\nprint('Load model')\nprediction_decoder = keras_cv.layers.NonMaxSuppression(bounding_box_format='xyxy', from_logits=False, confidence_threshold=0.2, iou_threshold=0.7)\nmodel = keras_cv.models.YOLOV8Detector.from_preset(preset='yolo_v8_xs_backbone_coco', num_classes=len(categories), load_weights=True, bounding_box_format='xyxy', prediction_decoder=prediction_decoder)\n# model.summary()","metadata":{"execution":{"iopub.status.busy":"2025-12-01T09:40:03.459316Z","iopub.execute_input":"2025-12-01T09:40:03.459911Z","iopub.status.idle":"2025-12-01T09:40:05.17346Z","shell.execute_reply.started":"2025-12-01T09:40:03.45989Z","shell.execute_reply":"2025-12-01T09:40:05.17266Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.optimizers import SGD\n\n# Learning rate is changed to 0.001\nopt = SGD(learning_rate=1e-3, momentum=0.9, global_clipnorm=10.0)\nmodel.compile(optimizer=opt, classification_loss='binary_crossentropy', box_loss='ciou', jit_compile=False)","metadata":{"execution":{"iopub.status.busy":"2025-12-01T09:40:05.174571Z","iopub.execute_input":"2025-12-01T09:40:05.174822Z","iopub.status.idle":"2025-12-01T09:40:05.185831Z","shell.execute_reply.started":"2025-12-01T09:40:05.174797Z","shell.execute_reply":"2025-12-01T09:40:05.185182Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.callbacks import TerminateOnNaN, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\n# Callbacks\nmodel_checkpoint = ModelCheckpoint('model.keras', monitor='val_loss', verbose=1, save_best_only=True)\nreduce_lr = ReduceLROnPlateau('val_loss', factor=0.1, patience=10, verbose=1)\nearly_stop = EarlyStopping('val_loss', patience=40, verbose=1)\nterminate = TerminateOnNaN()\ncallbacks = [model_checkpoint, reduce_lr, early_stop, terminate]","metadata":{"execution":{"iopub.status.busy":"2025-12-01T09:40:07.680747Z","iopub.execute_input":"2025-12-01T09:40:07.681711Z","iopub.status.idle":"2025-12-01T09:40:07.686153Z","shell.execute_reply.started":"2025-12-01T09:40:07.681675Z","shell.execute_reply":"2025-12-01T09:40:07.68546Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def image_generator(filename, tile, bboxes, categories):\n    def load_sample(filepath):\n        image = load_geoimage(filepath.numpy().decode('utf-8'))\n        return tf.cast(image, tf.uint8)\n    # Load image\n    img = tf.squeeze(tf.py_function(func=load_sample, inp=[filename], Tout=[tf.uint8]), axis=0)  # tf.print(tf.shape(img)) -> [H, W, 3]\n    img_roi = tf.image.pad_to_bounding_box(img, 0, 0, 640, 640)\n    return {'images': tf.cast(img_roi, tf.float32), 'bounding_boxes': {'boxes': bboxes, 'classes': categories}}\n\ndef ragged_to_dense(inputs):\n    from keras_cv import bounding_box\n    return {'images': inputs['images'].to_tensor(), 'bounding_boxes': bounding_box.to_dense(inputs['bounding_boxes'], max_boxes=2000)}\n\ndef dict_to_tuple(inputs):\n    return inputs['images'], inputs['bounding_boxes']","metadata":{"execution":{"iopub.status.busy":"2025-12-01T09:40:12.385951Z","iopub.execute_input":"2025-12-01T09:40:12.386657Z","iopub.status.idle":"2025-12-01T09:40:12.394163Z","shell.execute_reply.started":"2025-12-01T09:40:12.386631Z","shell.execute_reply":"2025-12-01T09:40:12.393354Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate the list of objects from annotations\nfilenames_train, tiles_train, bboxes_train, categories_train = zip(*list(map(lambda img_ann: (img_ann.filename, list(img_ann.tile), list([list(obj_ann.bb) for obj_ann in img_ann.objects]), list([list(categories.keys())[list(categories.values()).index(obj_ann.category)] for obj_ann in img_ann.objects])), anns_train)))\nfilenames_valid, tiles_valid, bboxes_valid, categories_valid = zip(*list(map(lambda img_ann: (img_ann.filename, list(img_ann.tile), list([list(obj_ann.bb) for obj_ann in img_ann.objects]), list([list(categories.keys())[list(categories.values()).index(obj_ann.category)] for obj_ann in img_ann.objects])), anns_valid)))\nds_train = tf.data.Dataset.from_tensor_slices((tf.cast(filenames_train, tf.string), tf.cast(tiles_train, tf.int32), tf.cast(tf.ragged.constant(bboxes_train), tf.float32).to_tensor(), tf.cast(tf.ragged.constant(categories_train), tf.float32).to_tensor()))\nds_valid = tf.data.Dataset.from_tensor_slices((tf.cast(filenames_valid, tf.string), tf.cast(tiles_valid, tf.int32), tf.cast(tf.ragged.constant(bboxes_valid), tf.float32).to_tensor(), tf.cast(tf.ragged.constant(categories_valid), tf.float32).to_tensor()))\nds_train = ds_train.map(image_generator, num_parallel_calls=tf.data.AUTOTUNE)\nds_valid = ds_valid.map(image_generator, num_parallel_calls=tf.data.AUTOTUNE)\n# Generators\nbatch_size = 4\nds_train = ds_train.shuffle(batch_size*5)\nds_train = ds_train.ragged_batch(batch_size=batch_size, drop_remainder=True)\nds_valid = ds_valid.ragged_batch(batch_size=batch_size, drop_remainder=True)\ndata_augmentation = tf.keras.Sequential(\n    layers=[keras_cv.layers.RandomFlip(mode='horizontal_and_vertical', bounding_box_format='xyxy'),\n            keras_cv.layers.RandomShear(x_factor=0.2, y_factor=0.2, bounding_box_format='xyxy'),\n            keras_cv.layers.RandomColorDegeneration(factor=0.5)])\nds_train = ds_train.map(data_augmentation, num_parallel_calls=tf.data.AUTOTUNE)\n# Bounding box tensors need to be Dense instead of Ragged\nds_train = ds_train.map(ragged_to_dense, num_parallel_calls=tf.data.AUTOTUNE)\nds_valid = ds_valid.map(ragged_to_dense, num_parallel_calls=tf.data.AUTOTUNE)\nds_train = ds_train.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\nds_valid = ds_valid.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2025-12-01T09:40:13.584947Z","iopub.execute_input":"2025-12-01T09:40:13.585696Z","iopub.status.idle":"2025-12-01T09:40:20.685185Z","shell.execute_reply.started":"2025-12-01T09:40:13.585671Z","shell.execute_reply":"2025-12-01T09:40:20.684596Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\nprint('Training model')\nepochs = 40\ntrain_steps, valid_steps = len(ds_train), len(ds_valid)\nh = model.fit(ds_train, steps_per_epoch=train_steps, validation_data=ds_valid, validation_steps=valid_steps, epochs=epochs, callbacks=callbacks, verbose=1)\n# Best validation model\nbest_idx = int(np.argmin(h.history['val_loss']))\nbest_value = np.min(h.history['val_loss'])\nprint('Best validation model: epoch ' + str(best_idx+1), ' - val_loss ' + str(best_value))","metadata":{"execution":{"iopub.execute_input":"2025-09-22T07:58:24.609634Z","iopub.status.busy":"2025-09-22T07:58:24.609484Z","iopub.status.idle":"2025-09-23T04:08:14.295657Z","shell.execute_reply":"2025-09-23T04:08:14.295107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(\"history_FT.json\", \"w\") as f:\n    json.dump(h.history, f)\n\nmodel.save(\"YOLOV8-1.keras\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Validation\nCompute validation metrics.","metadata":{"collapsed":false,"id":"pyX0YILvy-xa","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.colors as col\nimport numpy as np\n%matplotlib inline\n\ndef area_intersection(boxes, box):\n    xmin = np.maximum(np.min(boxes[:, 0::2], axis=1), np.min(box[0::2]))\n    ymin = np.maximum(np.min(boxes[:, 1::2], axis=1), np.min(box[1::2]))\n    xmax = np.minimum(np.max(boxes[:, 0::2], axis=1), np.max(box[0::2]))\n    ymax = np.minimum(np.max(boxes[:, 1::2], axis=1), np.max(box[1::2]))\n    w = np.maximum(xmax - xmin + 1.0, 0.0)\n    h = np.maximum(ymax - ymin + 1.0, 0.0)\n    return w * h\n\ndef area_union(boxes, box):\n    area_anns = (np.max(box[0::2])-np.min(box[0::2])+1.0) * (np.max(box[1::2])-np.min(box[1::2])+1.0)\n    area_pred = (np.max(boxes[:, 0::2], axis=1)-np.min(boxes[:, 0::2], axis=1)+1.0) * (np.max(boxes[:, 1::2], axis=1)-np.min(boxes[:, 1::2], axis=1)+1.0)\n    return area_anns + area_pred - area_intersection(boxes, box)\n\ndef calc_iou(boxes, box):\n    iou = area_intersection(boxes, box) / area_union(boxes, box)\n    max_value = np.max(iou)\n    max_index = np.argmax(iou)\n    return max_value, max_index\n\ndef calc_ap(rec, prec):\n    # First append sentinel values at the end\n    mrec = np.concatenate(([0.0], rec, [1.0]))\n    mpre = np.concatenate(([0.0], prec, [0.0]))\n    # Compute the precision envelope\n    for i in range(mpre.size-1, 0, -1):\n        mpre[i-1] = np.maximum(mpre[i-1], mpre[i])\n    # To calculate area under PR curve, look for points where X axis (recall) changes value\n    i = np.where(mrec[1:] != mrec[:-1])[0]\n    # and sum (\\Delta recall) * prec\n    ap = np.sum((mrec[i+1] - mrec[i]) * mpre[i+1])\n    return ap\n\ndef draw_confusion_matrix(cm, categories):\n    # Draw confusion matrix\n    fig = plt.figure(figsize=[6.4*pow(len(categories), 0.5), 4.8*pow(len(categories), 0.5)])\n    ax = fig.add_subplot(111)\n    cm = cm.astype('float') / np.maximum(cm.sum(axis=1)[:, np.newaxis], np.finfo(np.float64).eps)\n    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.get_cmap('Blues'))\n    ax.figure.colorbar(im, ax=ax)\n    ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]), xticklabels=categories, yticklabels=categories, ylabel='Annotation', xlabel='Prediction')\n    # Rotate the tick labels and set their alignment\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n    # Loop over data dimensions and create text annotations\n    thresh = cm.max() / 2.0\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], '.2f'), ha=\"center\", va=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=int(20-pow(len(categories), 0.5)))\n    fig.tight_layout()\n    plt.show()\n\ndef draw_precision_recall(precisions, recalls, categories):\n    # Draw precision-recall curves for each category\n    fig = plt.figure(figsize=[6.4*pow(len(categories), 0.5), 4.8*pow(len(categories), 0.5)])\n    ax = fig.add_subplot(111)\n    plt.axis([0, 1, 0, 1])\n    c_dark = list(filter(lambda x: x.startswith('dark'), col.cnames.keys()))\n    aps = []\n    # Compare categories for a specific algorithm\n    for idx in range(len(categories)):\n        plt.plot(recalls[idx], precisions[idx], color=c_dark[idx], label=categories[idx], linewidth=4.0)\n        aps.append(calc_ap(recalls[idx], precisions[idx]))\n    handles, labels = ax.get_legend_handles_labels()\n    labels = [str(val + ' [' + \"{:.3f}\".format(aps[idx]) + ']') for idx, val in enumerate(labels)]\n    handles = [h for (ap, h) in sorted(zip(aps, handles), key=lambda x: x[0], reverse=True)]\n    labels = [l for (ap, l) in sorted(zip(aps, labels), key=lambda x: x[0], reverse=True)]\n    leg = plt.legend(handles, labels, loc='upper right')\n    leg.set_zorder(100)\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.grid(\"on\", linestyle=\"--\", linewidth=2.0)\n    fig.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.execute_input":"2025-09-23T04:08:14.298013Z","iopub.status.busy":"2025-09-23T04:08:14.297846Z","iopub.status.idle":"2025-09-23T04:08:14.3711Z","shell.execute_reply":"2025-09-23T04:08:14.370016Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom tqdm import tqdm\n\n#model.load_weights('model.keras')\n# Generate the list of objects from annotations\nds_valid = tf.data.Dataset.from_tensor_slices((tf.cast(filenames_valid, tf.string), tf.cast(tiles_valid, tf.int32), tf.cast(tf.ragged.constant(bboxes_valid), tf.float32).to_tensor(), tf.cast(tf.ragged.constant(categories_valid), tf.float32).to_tensor()))\nds_valid = ds_valid.map(image_generator, num_parallel_calls=tf.data.AUTOTUNE).cache()\nds_valid = ds_valid.batch(batch_size=1)\nds_valid = ds_valid.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n# Process each tile sequentially\niterator = iter(ds_valid)\nannotations, predictions = {}, {}\nfor ann in tqdm(anns_valid):\n    # Save annotations\n    annotations.setdefault(ann.filename, {})\n    predictions.setdefault(ann.filename, {})\n    for obj in ann.objects:\n        annotations[ann.filename].setdefault(obj.category, {'bbox': []})\n        annotations[ann.filename][obj.category]['bbox'].append(obj.bb)\n    # Save prediction\n    image, _ = next(iterator)\n    y_pred = model.predict(image, verbose=0)\n    for i in range(np.squeeze(y_pred['num_detections'])):\n        obj = GenericObject()\n        bbox = np.squeeze(y_pred['boxes'])[i]\n        obj.bb = (bbox[0], bbox[1], bbox[2], bbox[3])\n        obj.category = categories[np.squeeze(y_pred['classes'])[i]]\n        obj.score = np.squeeze(y_pred['confidence'])[i]\n        predictions[ann.filename].setdefault(obj.category, {'bbox': [], 'confidence': []})\n        predictions[ann.filename][obj.category]['bbox'].append(obj.bb)\n        predictions[ann.filename][obj.category]['confidence'].append(obj.score)  # sort detections by confidence","metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2025-09-23T04:08:14.373872Z","iopub.status.busy":"2025-09-23T04:08:14.373705Z","iopub.status.idle":"2025-09-23T04:10:49.229845Z","shell.execute_reply":"2025-09-23T04:10:49.228874Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"threshold = 0.5\ndefault_cls = 'BACKGROUND'\ny_true, y_pred = [], []  # confusion matrix\ntps, confidences = dict(), dict()  # draw precision-recall curves for each category\nfor cls in categories.values():\n    # Compute TP, FP and FN for each image\n    tps[cls], confidences[cls] = [], []\n    for f in predictions:\n        # Sort 'cls' predictions by confidence for each file\n        pred_boxes, pred_confidences = [], []\n        if cls in predictions[f].keys():\n            for idx in range(len(predictions[f][cls]['bbox'])):\n                pred_boxes.append(predictions[f][cls]['bbox'][idx])\n                pred_confidences.append(predictions[f][cls]['confidence'][idx])\n            sorted_ind = np.argsort(-np.array(pred_confidences))\n            pred_boxes = np.array(pred_boxes)[sorted_ind, :]\n        pred_boxes = np.array(pred_boxes).astype(float)\n        # Define 'cls' annotations for each file\n        anno_boxes = []\n        if cls in annotations[f].keys():\n            anno_boxes = annotations[f][cls]['bbox']\n        anno_boxes = np.array(anno_boxes).astype(float)\n        # Define horizontal or oriented bounding boxes\n        anno_indices = list(range(len(anno_boxes)))\n        # Compare a single prediction 'pred_box' with all annotations 'anno_boxes'\n        for pred_idx, pred_box in enumerate(pred_boxes):\n            # A prediction is correct if its IoU with the ground truth is above the threshold\n            iou_value, ann_index = calc_iou(anno_boxes, pred_box) if len(anno_boxes) > 0 else (-1, -1)\n            if iou_value > threshold and ann_index in anno_indices:\n                # TP\n                anno_indices.remove(int(ann_index))\n                tps[cls] += [1.0]\n                y_true += [cls]\n            else:\n                # FP\n                tps[cls] += [0.0]\n                y_true += [default_cls]\n            y_pred += [cls]\n            confidences[cls] += [pred_confidences[pred_idx]]\n        # FN\n        y_true += [cls] * len(anno_indices)\n        y_pred += [default_cls] * len(anno_indices)\ny_true, y_pred = np.array(y_true), np.array(y_pred)","metadata":{"execution":{"iopub.execute_input":"2025-09-23T04:10:49.232029Z","iopub.status.busy":"2025-09-23T04:10:49.231877Z","iopub.status.idle":"2025-09-23T04:10:54.67472Z","shell.execute_reply":"2025-09-23T04:10:54.67377Z"},"id":"QsA9Dx57y-xf"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score\n\n# Compute AP metric\nprecision_list, recall_list, ap_list = [], [], []\nfor cls in categories.values():\n    sorted_ind = np.argsort(-np.array(confidences[cls]))\n    tp = np.cumsum(np.array(tps[cls])[sorted_ind], dtype=float)\n    recall = np.array([0.0]) if len(tp) == 0 else tp / np.maximum(np.sum(y_true == cls), np.finfo(np.float64).eps)\n    precision = np.array([0.0]) if len(tp) == 0 else tp / np.maximum(list(range(1, np.sum(y_pred == cls)+1)), np.finfo(np.float64).eps)\n    ap = calc_ap(recall, precision)\n    print('> %s: Recall: %.3f%% Precision: %.3f%% AP: %.3f%%' % (cls, recall[-1]*100, precision[-1]*100, ap*100))\n    precision_list.append(precision)\n    recall_list.append(recall)\n    ap_list.append(ap)\nmean_ap = np.mean(ap_list)\nprint('mAccuracy: %.3f%%' % (accuracy_score(y_true, y_pred)*100))\nprint('mRecall: %.3f%%' % (recall_score(y_true, y_pred, average='macro', zero_division=1)*100))\nprint('mPrecision: %.3f%%' % (precision_score(y_true, y_pred, average='macro', zero_division=1)*100))\nprint('mAP: %.3f%%' % (mean_ap*100))","metadata":{"execution":{"iopub.execute_input":"2025-09-23T04:10:54.676744Z","iopub.status.busy":"2025-09-23T04:10:54.676597Z","iopub.status.idle":"2025-09-23T04:10:55.027697Z","shell.execute_reply":"2025-09-23T04:10:55.027315Z"},"id":"6vXYDcQ0y-xh","outputId":"2b814533-4932-435c-c595-2d03ff96af52"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"names = list(categories.values()).copy()\nnames.insert(0, default_cls)\ncm = confusion_matrix(y_true, y_pred, labels=names)\nprint('Confusion matrix:')\nprint(cm)\ndraw_confusion_matrix(cm, names)\ndraw_precision_recall(precision_list, recall_list, categories)","metadata":{"execution":{"iopub.execute_input":"2025-09-23T04:10:55.02956Z","iopub.status.busy":"2025-09-23T04:10:55.029426Z","iopub.status.idle":"2025-09-23T04:10:55.640774Z","shell.execute_reply":"2025-09-23T04:10:55.639708Z"},"id":"P30Z5qLGy-xi","outputId":"62bf9fc1-bdfe-48a8-8060-d52fe3a81dab"},"outputs":[],"execution_count":null}]}