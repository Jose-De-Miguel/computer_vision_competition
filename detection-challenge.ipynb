{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13943031,"sourceType":"datasetVersion","datasetId":8886325},{"sourceId":6091,"sourceType":"modelInstanceVersion","modelInstanceId":4623,"modelId":2800},{"sourceId":6105,"sourceType":"modelInstanceVersion","modelInstanceId":4648,"modelId":2804},{"sourceId":6125,"sourceType":"modelInstanceVersion","modelInstanceId":4596,"modelId":2797},{"sourceId":1447,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":1220,"modelId":183}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/josmdemiguel/detection-challenge?scriptVersionId=284511495\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## Assignment: Object detection\n- Alumno 1: Jos√© Mar√≠a De Miguel\n- Alumno 2: Miguel √Ångel Rojo\n- Alumno 3: Sandra Mill√°n\n\nThe goals of the assignment are:\n* Put into practice acquired knowledge to detect and recognize objects of interest within a satellite image.\n\nTo address this problem, you must choose one of the following options:\n*\tImplement a sliding window strategy to process the whole image, and then train a classifier that determines whether each window includes or not an object of interest. In this way, you can use previous image classification model to infer the object category.\n*\tBuild a single-stage object detection model (e.g., YOLO, SSD, RetinaNet, etc.).\n*\tBuild a two-stage object detection model (e.g., Faster R-CNN, R-FCN, etc.).\n\nFollow the link below to download the detection data set ‚Äúxview_detection‚Äù: [https://drive.upm.es/s/P7nEf3Bygns7tbM](https://drive.upm.es/s/P7nEf3Bygns7tbM)","metadata":{"id":"zK490Ryuy-xN"}},{"cell_type":"code","source":"!pip install rasterio --quiet\n!pip install \"protobuf==3.20.*\" --quiet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T10:39:21.678693Z","iopub.execute_input":"2025-12-07T10:39:21.679448Z","iopub.status.idle":"2025-12-07T10:39:30.638393Z","shell.execute_reply.started":"2025-12-07T10:39:21.679421Z","shell.execute_reply":"2025-12-07T10:39:30.637368Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)\nprint(tf.config.list_physical_devices('GPU'))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:54:38.958447Z","iopub.execute_input":"2025-12-07T11:54:38.959142Z","iopub.status.idle":"2025-12-07T11:54:38.963526Z","shell.execute_reply.started":"2025-12-07T11:54:38.959114Z","shell.execute_reply":"2025-12-07T11:54:38.962743Z"}},"outputs":[{"name":"stdout","text":"2.18.0\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"import uuid\nimport numpy as np\n\nclass GenericObject:\n    \"\"\"\n    Generic object data.\n    \"\"\"\n    def __init__(self):\n        self.id = uuid.uuid4()\n        self.bb = (-1, -1, -1, -1)\n        self.category= -1\n        self.score = -1\n\nclass GenericImage:\n    \"\"\"\n    Generic image data.\n    \"\"\"\n    def __init__(self, filename):\n        self.filename = filename\n        self.tile = np.array([-1, -1, -1, -1])  # (pt_x, pt_y, pt_x+width, pt_y+height)\n        self.objects = list([])\n\n    def add_object(self, obj: GenericObject):\n        self.objects.append(obj)","metadata":{"ExecuteTime":{"end_time":"2023-11-21T15:41:41.841195340Z","start_time":"2023-11-21T15:41:41.832078114Z"},"execution":{"iopub.status.busy":"2025-12-07T10:39:30.64586Z","iopub.execute_input":"2025-12-07T10:39:30.646131Z","iopub.status.idle":"2025-12-07T10:39:30.661845Z","shell.execute_reply.started":"2025-12-07T10:39:30.646103Z","shell.execute_reply":"2025-12-07T10:39:30.661166Z"},"id":"TGvvuEREy-xT","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"categories = {0: 'Small car', 1: 'Bus', 2: 'Truck', 3: 'Building'}","metadata":{"execution":{"iopub.status.busy":"2025-12-07T10:39:30.663656Z","iopub.execute_input":"2025-12-07T10:39:30.663987Z","iopub.status.idle":"2025-12-07T10:39:30.676797Z","shell.execute_reply.started":"2025-12-07T10:39:30.663966Z","shell.execute_reply":"2025-12-07T10:39:30.676052Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nimport rasterio\nimport numpy as np\n\ndef load_geoimage(filename):\n    warnings.filterwarnings('ignore', category=rasterio.errors.NotGeoreferencedWarning)\n    src_raster = rasterio.open('/kaggle/input/xview-detection/' + filename, 'r')\n    # RasterIO to OpenCV (see inconsistencies between libjpeg and libjpeg-turbo)\n    input_type = src_raster.profile['dtype']\n    input_channels = src_raster.count\n    img = np.zeros((src_raster.height, src_raster.width, src_raster.count), dtype=input_type)\n    for band in range(input_channels):\n        img[:, :, band] = src_raster.read(band+1)\n    return img","metadata":{"execution":{"iopub.status.busy":"2025-12-07T10:39:30.677449Z","iopub.execute_input":"2025-12-07T10:39:30.677738Z","iopub.status.idle":"2025-12-07T10:39:31.182577Z","shell.execute_reply.started":"2025-12-07T10:39:30.677697Z","shell.execute_reply":"2025-12-07T10:39:31.181851Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Training\nDesign and train a detector to deal with the ‚Äúxview_detection‚Äù perception task.","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import json\n\n# Load database\njson_file = '/kaggle/input/xview-detection/xview_det_train.json'\nwith open(json_file) as ifs:\n    json_data = json.load(ifs)\nifs.close()","metadata":{"ExecuteTime":{"end_time":"2023-11-21T15:48:22.102317718Z","start_time":"2023-11-21T15:48:20.911767630Z"},"collapsed":false,"execution":{"iopub.status.busy":"2025-12-07T10:39:31.183428Z","iopub.execute_input":"2025-12-07T10:39:31.184214Z","iopub.status.idle":"2025-12-07T10:39:34.159632Z","shell.execute_reply.started":"2025-12-07T10:39:31.184184Z","shell.execute_reply":"2025-12-07T10:39:34.158884Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\ncounts = dict.fromkeys(categories.values(), 0)\nanns = []\nfor json_img in json_data['images'].values():\n    image = GenericImage(json_img['filename'])\n    image.tile = np.array([0, 0, json_img['width'], json_img['height']])\n    for json_ann in [elem for elem in json_data['annotations'].values() if elem['image_id'] == json_img['image_id']]:\n        obj = GenericObject()\n        obj.id = json_ann['image_id']\n        obj.bb = (int(json_ann['bbox'][0]), int(json_ann['bbox'][1]), int(json_ann['bbox'][2]), int(json_ann['bbox'][3]))\n        obj.category = json_ann['category_id']\n        counts[obj.category] += 1\n        image.add_object(obj)\n    anns.append(image)\nprint(counts)","metadata":{"execution":{"iopub.status.busy":"2025-12-07T10:39:34.160571Z","iopub.execute_input":"2025-12-07T10:39:34.160878Z","iopub.status.idle":"2025-12-07T10:45:15.129572Z","shell.execute_reply.started":"2025-12-07T10:39:34.160847Z","shell.execute_reply":"2025-12-07T10:45:15.128897Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nanns_train, anns_valid = train_test_split(anns, test_size=0.1, random_state=1, shuffle=True)\nprint('Number of training images: ' + str(len(anns_train)))\nprint('Number of validation images: ' + str(len(anns_valid)))","metadata":{"execution":{"iopub.status.busy":"2025-12-07T10:45:15.130451Z","iopub.execute_input":"2025-12-07T10:45:15.130751Z","iopub.status.idle":"2025-12-07T10:45:15.236429Z","shell.execute_reply.started":"2025-12-07T10:45:15.130711Z","shell.execute_reply":"2025-12-07T10:45:15.235761Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import keras_cv\nimport inspect\n\nprint(keras_cv.__version__)\nprint(dir(keras_cv.models))  # ver qu√© modelos hay realmente","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:15:55.571996Z","iopub.execute_input":"2025-12-07T11:15:55.572666Z","iopub.status.idle":"2025-12-07T11:15:55.577169Z","shell.execute_reply.started":"2025-12-07T11:15:55.572642Z","shell.execute_reply":"2025-12-07T11:15:55.576179Z"}},"outputs":[{"name":"stdout","text":"0.9.0\n['BASNet', 'Backbone', 'CLIP', 'CSPDarkNetBackbone', 'CSPDarkNetLBackbone', 'CSPDarkNetMBackbone', 'CSPDarkNetSBackbone', 'CSPDarkNetTinyBackbone', 'CSPDarkNetXLBackbone', 'CenterPillarBackbone', 'DeepLabV3Plus', 'DenseNet121Backbone', 'DenseNet169Backbone', 'DenseNet201Backbone', 'DenseNetBackbone', 'EfficientNetLiteB0Backbone', 'EfficientNetLiteB1Backbone', 'EfficientNetLiteB2Backbone', 'EfficientNetLiteB3Backbone', 'EfficientNetLiteB4Backbone', 'EfficientNetLiteBackbone', 'EfficientNetV1B0Backbone', 'EfficientNetV1B1Backbone', 'EfficientNetV1B2Backbone', 'EfficientNetV1B3Backbone', 'EfficientNetV1B4Backbone', 'EfficientNetV1B5Backbone', 'EfficientNetV1B6Backbone', 'EfficientNetV1B7Backbone', 'EfficientNetV1Backbone', 'EfficientNetV2B0Backbone', 'EfficientNetV2B1Backbone', 'EfficientNetV2B2Backbone', 'EfficientNetV2B3Backbone', 'EfficientNetV2Backbone', 'EfficientNetV2LBackbone', 'EfficientNetV2MBackbone', 'EfficientNetV2SBackbone', 'ImageClassifier', 'MiTB0Backbone', 'MiTB1Backbone', 'MiTB2Backbone', 'MiTB3Backbone', 'MiTB4Backbone', 'MiTB5Backbone', 'MiTBackbone', 'MobileNetV3Backbone', 'MobileNetV3LargeBackbone', 'MobileNetV3SmallBackbone', 'MultiHeadCenterPillar', 'ResNet101Backbone', 'ResNet101V2Backbone', 'ResNet152Backbone', 'ResNet152V2Backbone', 'ResNet18Backbone', 'ResNet18V2Backbone', 'ResNet34Backbone', 'ResNet34V2Backbone', 'ResNet50Backbone', 'ResNet50V2Backbone', 'ResNetBackbone', 'ResNetV2Backbone', 'RetinaNet', 'SAMMaskDecoder', 'SAMPromptEncoder', 'SegFormer', 'SegFormerB0', 'SegFormerB1', 'SegFormerB2', 'SegFormerB3', 'SegFormerB4', 'SegFormerB5', 'SegmentAnythingModel', 'StableDiffusion', 'StableDiffusionV2', 'Task', 'TwoWayTransformer', 'VGG16Backbone', 'ViTDetBBackbone', 'ViTDetBackbone', 'ViTDetHBackbone', 'ViTDetLBackbone', 'VideoClassifier', 'VideoSwinBBackbone', 'VideoSwinBackbone', 'VideoSwinSBackbone', 'VideoSwinTBackbone', 'YOLOV8Backbone', 'YOLOV8Detector', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'classification', 'feature_extractor', 'object_detection', 'retinanet', 'segmentation', 'stable_diffusion', 'yolov8']\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"import keras_cv\n\nprint(\"ResNetBackbone presets:\", getattr(keras_cv.models.ResNetBackbone, \"presets\", {}).keys())\nprint(\"ResNet50Backbone presets:\", getattr(keras_cv.models.ResNet50Backbone, \"presets\", {}).keys())\nprint(\"EfficientNetV2Backbone presets:\", getattr(keras_cv.models.EfficientNetV2Backbone, \"presets\", {}).keys())\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:15:55.894687Z","iopub.execute_input":"2025-12-07T11:15:55.89532Z","iopub.status.idle":"2025-12-07T11:15:55.900396Z","shell.execute_reply.started":"2025-12-07T11:15:55.895296Z","shell.execute_reply":"2025-12-07T11:15:55.899656Z"}},"outputs":[{"name":"stdout","text":"ResNetBackbone presets: dict_keys(['resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152', 'resnet50_imagenet'])\nResNet50Backbone presets: dict_keys(['resnet50_imagenet'])\nEfficientNetV2Backbone presets: dict_keys(['efficientnetv2_s', 'efficientnetv2_m', 'efficientnetv2_l', 'efficientnetv2_b0', 'efficientnetv2_b1', 'efficientnetv2_b2', 'efficientnetv2_b3', 'efficientnetv2_s_imagenet', 'efficientnetv2_b0_imagenet', 'efficientnetv2_b1_imagenet', 'efficientnetv2_b2_imagenet'])\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"from keras_cv import bounding_box\n\nIMAGE_SIZE = 640\n\ndef image_generator(filename, tile, bboxes, categories):\n    def load_sample(filepath):\n        image = load_geoimage(filepath.numpy().decode('utf-8'))\n        return tf.cast(image, tf.uint8)\n\n    # 1. Cargar con py_function\n    img = tf.squeeze(\n        tf.py_function(func=load_sample, inp=[filename], Tout=[tf.uint8]),\n        axis=0\n    )\n    img.set_shape([None, None, 3])\n\n    # 2. Redimensionar / pad a IMAGE_SIZE x IMAGE_SIZE\n    img_roi = tf.image.pad_to_bounding_box(img, 0, 0, IMAGE_SIZE, IMAGE_SIZE)\n    img_roi.set_shape([IMAGE_SIZE, IMAGE_SIZE, 3])\n\n    # 3. Normalizar\n    img_roi = tf.cast(img_roi, tf.float32) / 255.0\n\n    # ‚ö†Ô∏è Asegurar tipos y formato\n    bboxes = tf.cast(bboxes, tf.float32)     # (num_boxes, 4), xyxy\n    categories = tf.cast(categories, tf.int32)  # clases enteras 0..NUM_CLASSES-1\n\n    return {\n        \"images\": img_roi,\n        \"bounding_boxes\": {\n            \"boxes\": bboxes,\n            \"classes\": categories,\n            \"bounding_box_format\": \"xyxy\",  # üëà MUY importante\n        },\n    }\n\n\n\ndef dict_to_tuple(inputs):\n    # inputs: dict con \"images\" y \"bounding_boxes\"\n    images = inputs[\"images\"]\n    dense_bboxes = bounding_box.to_dense(\n        inputs[\"bounding_boxes\"],\n        max_boxes=100,\n    )\n    # dense_bboxes sigue siendo un dict {\"boxes\": ..., \"classes\": ..., \"bounding_box_format\": ...}\n    return images, dense_bboxes\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:01:16.090892Z","iopub.execute_input":"2025-12-07T11:01:16.091582Z","iopub.status.idle":"2025-12-07T11:01:16.098557Z","shell.execute_reply.started":"2025-12-07T11:01:16.091557Z","shell.execute_reply":"2025-12-07T11:01:16.097858Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate the list of objects from annotations\nfilenames_train, tiles_train, bboxes_train, categories_train = zip(*list(map(lambda img_ann: (img_ann.filename, list(img_ann.tile), list([list(obj_ann.bb) for obj_ann in img_ann.objects]), list([list(categories.keys())[list(categories.values()).index(obj_ann.category)] for obj_ann in img_ann.objects])), anns_train)))\nfilenames_valid, tiles_valid, bboxes_valid, categories_valid = zip(*list(map(lambda img_ann: (img_ann.filename, list(img_ann.tile), list([list(obj_ann.bb) for obj_ann in img_ann.objects]), list([list(categories.keys())[list(categories.values()).index(obj_ann.category)] for obj_ann in img_ann.objects])), anns_valid)))\nds_train = tf.data.Dataset.from_tensor_slices((tf.cast(filenames_train, tf.string), tf.cast(tiles_train, tf.int32), tf.cast(tf.ragged.constant(bboxes_train), tf.float32).to_tensor(), tf.cast(tf.ragged.constant(categories_train), tf.float32).to_tensor()))\nds_valid = tf.data.Dataset.from_tensor_slices((tf.cast(filenames_valid, tf.string), tf.cast(tiles_valid, tf.int32), tf.cast(tf.ragged.constant(bboxes_valid), tf.float32).to_tensor(), tf.cast(tf.ragged.constant(categories_valid), tf.float32).to_tensor()))\n# ds_train / ds_valid creados como t√∫\n\nds_train = ds_train.map(image_generator, num_parallel_calls=tf.data.AUTOTUNE)\nds_valid = ds_valid.map(image_generator, num_parallel_calls=tf.data.AUTOTUNE)\n\nbatch_size = 4\n\n# Mezclar y hacer batch\nds_train = ds_train.shuffle(batch_size * 5)\nds_train = ds_train.ragged_batch(batch_size=batch_size, drop_remainder=True)\nds_valid = ds_valid.ragged_batch(batch_size=batch_size, drop_remainder=True)\n\n# Data augmentation que acepta dict con images+bounding_boxes\ndata_augmentation = tf.keras.Sequential(\n    layers=[\n        keras_cv.layers.RandomFlip(\n            mode='horizontal_and_vertical',\n            bounding_box_format='xyxy',\n        ),\n        keras_cv.layers.RandomShear(\n            x_factor=0.2,\n            y_factor=0.2,\n            bounding_box_format='xyxy',\n        ),\n        keras_cv.layers.RandomColorDegeneration(factor=0.5),\n    ]\n)\n\nds_train = ds_train.map(data_augmentation, num_parallel_calls=tf.data.AUTOTUNE)\n\n# Densificar SOLO las bounding boxes\nds_train = ds_train.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\nds_valid = ds_valid.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n\nds_train = ds_train.prefetch(tf.data.AUTOTUNE)\nds_valid = ds_valid.prefetch(tf.data.AUTOTUNE)\n","metadata":{"execution":{"iopub.status.busy":"2025-12-07T11:01:16.81256Z","iopub.execute_input":"2025-12-07T11:01:16.812879Z","iopub.status.idle":"2025-12-07T11:01:22.599993Z","shell.execute_reply.started":"2025-12-07T11:01:16.812856Z","shell.execute_reply":"2025-12-07T11:01:22.599212Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds_train.take(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:01:22.601151Z","iopub.execute_input":"2025-12-07T11:01:22.601373Z","iopub.status.idle":"2025-12-07T11:01:22.607421Z","shell.execute_reply.started":"2025-12-07T11:01:22.601355Z","shell.execute_reply":"2025-12-07T11:01:22.606833Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"NUM_CLASSES = len(categories)\nIMAGE_SIZE = 640\nNUM_EPOCHS = 40\nBBOX_FORMAT = \"xyxy\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:01:22.608165Z","iopub.execute_input":"2025-12-07T11:01:22.60839Z","iopub.status.idle":"2025-12-07T11:01:22.620494Z","shell.execute_reply.started":"2025-12-07T11:01:22.608368Z","shell.execute_reply":"2025-12-07T11:01:22.619942Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import keras_cv\nfrom tensorflow import keras\n\n\n\n\n# (Opcional) NMS extra si quieres post-filtrar resultados\nprediction_decoder = keras_cv.layers.NonMaxSuppression(\n    bounding_box_format='xyxy',\n    from_logits=False,\n    confidence_threshold=0.2,\n    iou_threshold=0.7,\n)\n\n\n\n\nmodel = keras_cv.models.RetinaNet.from_preset(\n    \"resnet50_imagenet\",\n    num_classes=NUM_CLASSES,\n    bounding_box_format=BBOX_FORMAT,\n    # opcional:\n    # load_weights=False,  # si quieres empezar desde cero\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2025-12-07T11:01:23.502224Z","iopub.execute_input":"2025-12-07T11:01:23.502526Z","iopub.status.idle":"2025-12-07T11:01:27.010345Z","shell.execute_reply.started":"2025-12-07T11:01:23.502501Z","shell.execute_reply":"2025-12-07T11:01:27.009718Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow import keras\nimport keras_cv\n\nsteps_per_epoch = len(ds_train)  # pon aqu√≠ tu valor real\nepochs = NUM_EPOCHS\n\nbase_lr = 1e-4\n\nlr_schedule = keras.optimizers.schedules.CosineDecay(\n    initial_learning_rate=base_lr,\n    decay_steps=steps_per_epoch * epochs,\n    alpha=0.1,  # LR final = 10% de la inicial\n)\n\nopt = keras.optimizers.AdamW(\n    learning_rate=lr_schedule,\n    weight_decay=5e-4,\n    global_clipnorm=10.0,\n)\n\nclassification_loss = keras_cv.losses.FocalLoss(\n    from_logits=True,\n    alpha=0.25,\n    gamma=2.0,\n)\n\nbox_loss = keras_cv.losses.SmoothL1Loss(\n    l1_cutoff=0.1,                 # <- este es el par√°metro correcto en 0.9.0\n    reduction=\"sum_over_batch_size\",\n)\n\n\nmodel.compile(\n    optimizer=opt,\n    classification_loss=classification_loss,\n    box_loss=box_loss,\n    jit_compile=False,  # si te va estable, puedes probar True para m√°s velocidad\n)\n","metadata":{"execution":{"iopub.status.busy":"2025-12-07T11:01:27.011488Z","iopub.execute_input":"2025-12-07T11:01:27.011767Z","iopub.status.idle":"2025-12-07T11:01:27.021671Z","shell.execute_reply.started":"2025-12-07T11:01:27.011744Z","shell.execute_reply":"2025-12-07T11:01:27.0211Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.callbacks import TerminateOnNaN, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\n# Callbacks\nmodel_checkpoint = ModelCheckpoint('/kaggle/working/R-CNN-training1.keras', monitor='val_loss', verbose=1, save_best_only=True)\n# no usar con cosine decay: reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1, patience=10, verbose=1)\nearly_stop = EarlyStopping('val_loss', patience=10, verbose=1)\nterminate = TerminateOnNaN()\ncallbacks = [model_checkpoint, early_stop, terminate]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:01:30.731785Z","iopub.execute_input":"2025-12-07T11:01:30.732472Z","iopub.status.idle":"2025-12-07T11:01:30.737209Z","shell.execute_reply.started":"2025-12-07T11:01:30.73245Z","shell.execute_reply":"2025-12-07T11:01:30.736613Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for x, y in ds_train.take(1):\n    print(\"images:\", x.shape)\n    print(\"boxes:\", y[\"boxes\"].shape)\n    print(\"classes:\", y[\"classes\"].shape)\n    break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T11:01:30.965342Z","iopub.execute_input":"2025-12-07T11:01:30.967336Z","iopub.status.idle":"2025-12-07T11:01:32.313315Z","shell.execute_reply.started":"2025-12-07T11:01:30.967298Z","shell.execute_reply":"2025-12-07T11:01:32.312471Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\nprint('Training model')\nepochs = NUM_EPOCHS\ntrain_steps, valid_steps = len(ds_train), len(ds_valid)\n\nh = model.fit(\n    ds_train,\n    steps_per_epoch=train_steps,\n    validation_data=ds_valid,\n    validation_steps=valid_steps, \n    epochs=NUM_EPOCHS,\n    callbacks=callbacks,\n    verbose=1,\n)\n# Best validation model\nbest_idx = int(np.argmin(h.history['val_loss']))\nbest_value = np.min(h.history['val_loss'])\nprint('Best validation model: epoch ' + str(best_idx+1), ' - val_loss ' + str(best_value))","metadata":{"execution":{"iopub.status.busy":"2025-12-07T11:01:32.315842Z","iopub.execute_input":"2025-12-07T11:01:32.316107Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(\"RetinaNet-1.json\", \"w\") as f:\n    json.dump(h.history, f)\n\nmodel.save(\"RetinaNet-1.keras\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T10:45:37.519223Z","iopub.status.idle":"2025-12-07T10:45:37.519451Z","shell.execute_reply.started":"2025-12-07T10:45:37.519343Z","shell.execute_reply":"2025-12-07T10:45:37.519354Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Validation\nCompute validation metrics.","metadata":{"collapsed":false,"id":"pyX0YILvy-xa","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.colors as col\nimport numpy as np\n%matplotlib inline\n\ndef area_intersection(boxes, box):\n    xmin = np.maximum(np.min(boxes[:, 0::2], axis=1), np.min(box[0::2]))\n    ymin = np.maximum(np.min(boxes[:, 1::2], axis=1), np.min(box[1::2]))\n    xmax = np.minimum(np.max(boxes[:, 0::2], axis=1), np.max(box[0::2]))\n    ymax = np.minimum(np.max(boxes[:, 1::2], axis=1), np.max(box[1::2]))\n    w = np.maximum(xmax - xmin + 1.0, 0.0)\n    h = np.maximum(ymax - ymin + 1.0, 0.0)\n    return w * h\n\ndef area_union(boxes, box):\n    area_anns = (np.max(box[0::2])-np.min(box[0::2])+1.0) * (np.max(box[1::2])-np.min(box[1::2])+1.0)\n    area_pred = (np.max(boxes[:, 0::2], axis=1)-np.min(boxes[:, 0::2], axis=1)+1.0) * (np.max(boxes[:, 1::2], axis=1)-np.min(boxes[:, 1::2], axis=1)+1.0)\n    return area_anns + area_pred - area_intersection(boxes, box)\n\ndef calc_iou(boxes, box):\n    iou = area_intersection(boxes, box) / area_union(boxes, box)\n    max_value = np.max(iou)\n    max_index = np.argmax(iou)\n    return max_value, max_index\n\ndef calc_ap(rec, prec):\n    # First append sentinel values at the end\n    mrec = np.concatenate(([0.0], rec, [1.0]))\n    mpre = np.concatenate(([0.0], prec, [0.0]))\n    # Compute the precision envelope\n    for i in range(mpre.size-1, 0, -1):\n        mpre[i-1] = np.maximum(mpre[i-1], mpre[i])\n    # To calculate area under PR curve, look for points where X axis (recall) changes value\n    i = np.where(mrec[1:] != mrec[:-1])[0]\n    # and sum (\\Delta recall) * prec\n    ap = np.sum((mrec[i+1] - mrec[i]) * mpre[i+1])\n    return ap\n\ndef draw_confusion_matrix(cm, categories):\n    # Draw confusion matrix\n    fig = plt.figure(figsize=[6.4*pow(len(categories), 0.5), 4.8*pow(len(categories), 0.5)])\n    ax = fig.add_subplot(111)\n    cm = cm.astype('float') / np.maximum(cm.sum(axis=1)[:, np.newaxis], np.finfo(np.float64).eps)\n    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.get_cmap('Blues'))\n    ax.figure.colorbar(im, ax=ax)\n    ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]), xticklabels=categories, yticklabels=categories, ylabel='Annotation', xlabel='Prediction')\n    # Rotate the tick labels and set their alignment\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n    # Loop over data dimensions and create text annotations\n    thresh = cm.max() / 2.0\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], '.2f'), ha=\"center\", va=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=int(20-pow(len(categories), 0.5)))\n    fig.tight_layout()\n    plt.show()\n\ndef draw_precision_recall(precisions, recalls, categories):\n    # Draw precision-recall curves for each category\n    fig = plt.figure(figsize=[6.4*pow(len(categories), 0.5), 4.8*pow(len(categories), 0.5)])\n    ax = fig.add_subplot(111)\n    plt.axis([0, 1, 0, 1])\n    c_dark = list(filter(lambda x: x.startswith('dark'), col.cnames.keys()))\n    aps = []\n    # Compare categories for a specific algorithm\n    for idx in range(len(categories)):\n        plt.plot(recalls[idx], precisions[idx], color=c_dark[idx], label=categories[idx], linewidth=4.0)\n        aps.append(calc_ap(recalls[idx], precisions[idx]))\n    handles, labels = ax.get_legend_handles_labels()\n    labels = [str(val + ' [' + \"{:.3f}\".format(aps[idx]) + ']') for idx, val in enumerate(labels)]\n    handles = [h for (ap, h) in sorted(zip(aps, handles), key=lambda x: x[0], reverse=True)]\n    labels = [l for (ap, l) in sorted(zip(aps, labels), key=lambda x: x[0], reverse=True)]\n    leg = plt.legend(handles, labels, loc='upper right')\n    leg.set_zorder(100)\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.grid(\"on\", linestyle=\"--\", linewidth=2.0)\n    fig.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2025-12-07T10:45:37.520902Z","iopub.status.idle":"2025-12-07T10:45:37.521471Z","shell.execute_reply.started":"2025-12-07T10:45:37.521317Z","shell.execute_reply":"2025-12-07T10:45:37.521336Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom tqdm import tqdm\n\n#model.load_weights('model.keras')\n# Generate the list of objects from annotations\nds_valid = tf.data.Dataset.from_tensor_slices((tf.cast(filenames_valid, tf.string), tf.cast(tiles_valid, tf.int32), tf.cast(tf.ragged.constant(bboxes_valid), tf.float32).to_tensor(), tf.cast(tf.ragged.constant(categories_valid), tf.float32).to_tensor()))\nds_valid = ds_valid.map(image_generator, num_parallel_calls=tf.data.AUTOTUNE).cache()\nds_valid = ds_valid.batch(batch_size=1)\nds_valid = ds_valid.prefetch(tf.data.AUTOTUNE)\n# Process each tile sequentially\niterator = iter(ds_valid)\nannotations, predictions = {}, {}\nfor ann in tqdm(anns_valid):\n    # Save annotations\n    annotations.setdefault(ann.filename, {})\n    predictions.setdefault(ann.filename, {})\n    for obj in ann.objects:\n        annotations[ann.filename].setdefault(obj.category, {'bbox': []})\n        annotations[ann.filename][obj.category]['bbox'].append(obj.bb)\n    # Save prediction\n    image, _ = next(iterator)\n    y_pred = model.predict(image, verbose=0)\n    for i in range(np.squeeze(y_pred['num_detections'])):\n        obj = GenericObject()\n        bbox = np.squeeze(y_pred['boxes'])[i]\n        obj.bb = (bbox[0], bbox[1], bbox[2], bbox[3])\n        obj.category = categories[np.squeeze(y_pred['classes'])[i]]\n        obj.score = np.squeeze(y_pred['confidence'])[i]\n        predictions[ann.filename].setdefault(obj.category, {'bbox': [], 'confidence': []})\n        predictions[ann.filename][obj.category]['bbox'].append(obj.bb)\n        predictions[ann.filename][obj.category]['confidence'].append(obj.score)  # sort detections by confidence","metadata":{"collapsed":false,"execution":{"iopub.status.busy":"2025-12-07T10:45:37.522466Z","iopub.status.idle":"2025-12-07T10:45:37.523062Z","shell.execute_reply.started":"2025-12-07T10:45:37.522869Z","shell.execute_reply":"2025-12-07T10:45:37.522886Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"threshold = 0.5\ndefault_cls = 'BACKGROUND'\ny_true, y_pred = [], []  # confusion matrix\ntps, confidences = dict(), dict()  # draw precision-recall curves for each category\nfor cls in categories.values():\n    # Compute TP, FP and FN for each image\n    tps[cls], confidences[cls] = [], []\n    for f in predictions:\n        # Sort 'cls' predictions by confidence for each file\n        pred_boxes, pred_confidences = [], []\n        if cls in predictions[f].keys():\n            for idx in range(len(predictions[f][cls]['bbox'])):\n                pred_boxes.append(predictions[f][cls]['bbox'][idx])\n                pred_confidences.append(predictions[f][cls]['confidence'][idx])\n            sorted_ind = np.argsort(-np.array(pred_confidences))\n            pred_boxes = np.array(pred_boxes)[sorted_ind, :]\n        pred_boxes = np.array(pred_boxes).astype(float)\n        # Define 'cls' annotations for each file\n        anno_boxes = []\n        if cls in annotations[f].keys():\n            anno_boxes = annotations[f][cls]['bbox']\n        anno_boxes = np.array(anno_boxes).astype(float)\n        # Define horizontal or oriented bounding boxes\n        anno_indices = list(range(len(anno_boxes)))\n        # Compare a single prediction 'pred_box' with all annotations 'anno_boxes'\n        for pred_idx, pred_box in enumerate(pred_boxes):\n            # A prediction is correct if its IoU with the ground truth is above the threshold\n            iou_value, ann_index = calc_iou(anno_boxes, pred_box) if len(anno_boxes) > 0 else (-1, -1)\n            if iou_value > threshold and ann_index in anno_indices:\n                # TP\n                anno_indices.remove(int(ann_index))\n                tps[cls] += [1.0]\n                y_true += [cls]\n            else:\n                # FP\n                tps[cls] += [0.0]\n                y_true += [default_cls]\n            y_pred += [cls]\n            confidences[cls] += [pred_confidences[pred_idx]]\n        # FN\n        y_true += [cls] * len(anno_indices)\n        y_pred += [default_cls] * len(anno_indices)\ny_true, y_pred = np.array(y_true), np.array(y_pred)","metadata":{"execution":{"iopub.status.busy":"2025-12-07T10:45:37.524184Z","iopub.status.idle":"2025-12-07T10:45:37.524587Z","shell.execute_reply.started":"2025-12-07T10:45:37.524411Z","shell.execute_reply":"2025-12-07T10:45:37.524428Z"},"id":"QsA9Dx57y-xf","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score\n\n# Compute AP metric\nprecision_list, recall_list, ap_list = [], [], []\nfor cls in categories.values():\n    sorted_ind = np.argsort(-np.array(confidences[cls]))\n    tp = np.cumsum(np.array(tps[cls])[sorted_ind], dtype=float)\n    recall = np.array([0.0]) if len(tp) == 0 else tp / np.maximum(np.sum(y_true == cls), np.finfo(np.float64).eps)\n    precision = np.array([0.0]) if len(tp) == 0 else tp / np.maximum(list(range(1, np.sum(y_pred == cls)+1)), np.finfo(np.float64).eps)\n    ap = calc_ap(recall, precision)\n    print('> %s: Recall: %.3f%% Precision: %.3f%% AP: %.3f%%' % (cls, recall[-1]*100, precision[-1]*100, ap*100))\n    precision_list.append(precision)\n    recall_list.append(recall)\n    ap_list.append(ap)\nmean_ap = np.mean(ap_list)\nprint('mAccuracy: %.3f%%' % (accuracy_score(y_true, y_pred)*100))\nprint('mRecall: %.3f%%' % (recall_score(y_true, y_pred, average='macro', zero_division=1)*100))\nprint('mPrecision: %.3f%%' % (precision_score(y_true, y_pred, average='macro', zero_division=1)*100))\nprint('mAP: %.3f%%' % (mean_ap*100))","metadata":{"execution":{"iopub.status.busy":"2025-12-07T10:45:37.525628Z","iopub.status.idle":"2025-12-07T10:45:37.52589Z","shell.execute_reply.started":"2025-12-07T10:45:37.525768Z","shell.execute_reply":"2025-12-07T10:45:37.525782Z"},"id":"6vXYDcQ0y-xh","outputId":"2b814533-4932-435c-c595-2d03ff96af52","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"names = list(categories.values()).copy()\nnames.insert(0, default_cls)\ncm = confusion_matrix(y_true, y_pred, labels=names)\nprint('Confusion matrix:')\nprint(cm)\ndraw_confusion_matrix(cm, names)\ndraw_precision_recall(precision_list, recall_list, categories)","metadata":{"execution":{"iopub.status.busy":"2025-12-07T10:45:37.527164Z","iopub.status.idle":"2025-12-07T10:45:37.52748Z","shell.execute_reply.started":"2025-12-07T10:45:37.527322Z","shell.execute_reply":"2025-12-07T10:45:37.527336Z"},"id":"P30Z5qLGy-xi","outputId":"62bf9fc1-bdfe-48a8-8060-d52fe3a81dab","trusted":true},"outputs":[],"execution_count":null}]}